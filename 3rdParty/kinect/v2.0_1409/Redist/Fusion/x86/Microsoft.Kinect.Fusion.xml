<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.Kinect.Fusion</name>
    </assembly>
    <members>
        <member name="T:Microsoft.Kinect.Fusion.CameraParameters">
            <summary>
            This class is used to store the intrinsic camera parameters.
            These parameters describe the optical system of the camera lens and sensor.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraParameters.CameraDepthNominalFocalLengthInPixels">
            <summary>
            The Kinect For Windows parameters.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraParameters.DepthNormFocalLengthX">
            <summary>
            The x value of the default normalized focal length.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraParameters.DepthNormFocalLengthY">
            <summary>
            The y value of the default normalized focal length.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraParameters.DepthNormPrincipalPointX">
            <summary>
            The x value of the depth normalized principal point.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraParameters.DepthNormPrincipalPointY">
            <summary>
            The y value of the depth normalized principal point.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraParameters.defaultCameraParameters">
            <summary>
            The private member variable to cache the default camera parameters.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraParameters.#ctor(System.Single,System.Single,System.Single,System.Single)">
            <summary>
            Initializes a new instance of the CameraParameters class.
            </summary>
            <param name="focalLengthX">The focal length for X normalized by the camera width.</param>
            <param name="focalLengthY">The focal length for Y normalized by the camera height.</param>
            <param name="principalPointX">The principal point for X normalized by the camera width.</param>
            <param name="principalPointY">The principal point for Y normalized by the camera height.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraParameters.#ctor(Microsoft.Kinect.KinectSensor)">
            <summary>
            Initializes a new instance of the CameraParameters class.
            </summary>
            <param name="sensor">The Kinect sensor to use to initialize the camera parameters.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraParameters.#ctor">
            <summary>
            Prevents a default instance of the CameraParameters class from being created.
            The declaration of the default constructor is used for marshaling operations.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraParameters.GetHashCode">
            <summary>
            Calculates the hash code of the CameraParameters.
            </summary>
            <returns>The hash code.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraParameters.Equals(Microsoft.Kinect.Fusion.CameraParameters)">
            <summary>
            Determines if the two objects are equal.
            </summary>
            <param name="other">The object to compare to.</param>
            <returns>This method returns true if they are equal and false otherwise.</returns>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.CameraParameters.Defaults">
            <summary>
            Gets the default parameters.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.CameraParameters.FocalLengthX">
            <summary>
            Gets the focal length for X normalized by the camera width.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.CameraParameters.FocalLengthY">
            <summary>
            Gets the focal length for Y normalized by the camera height.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.CameraParameters.PrincipalPointX">
            <summary>
            Gets the principal point for X normalized by the camera width.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.CameraParameters.PrincipalPointY">
            <summary>
            Gets the principal point for Y normalized by the camera height.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.CameraPoseFinder">
            <summary>
            CameraPoseFinder encapsulates camera pose finder creation, updating and pose-finding functions.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraPoseFinder.DefaultMinimumDistanceThreshold">
            <summary>
            The default minimum distance threshold for capturing key frame poses for use in ProcessFrame.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraPoseFinder.cameraPoseFinder">
            <summary>
            The native CameraPoseFinder interface wrapper.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraPoseFinder.disposed">
            <summary>
            Track whether Dispose has been called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinder.#ctor(Microsoft.Kinect.Fusion.INuiFusionCameraPoseFinder)">
            <summary>
            Initializes a new instance of the CameraPoseFinder class.
            Default constructor used to initialize with the native CameraPoseFinder object.
            </summary>
            <param name="poseFinder">
            The native CameraPoseFinder object to be encapsulated.
            </param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinder.Finalize">
            <summary>
            Finalizes an instance of the CameraPoseFinder class.
            This destructor will run only if the Dispose method does not get called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinder.FusionCreateCameraPoseFinder(Microsoft.Kinect.Fusion.CameraPoseFinderParameters)">
            <summary>
            Initialize a Kinect Fusion cameraPoseFinder.
            A Kinect camera is required to be connected.
            </summary>
            <param name="cameraPoseFinderParameters">
            The parameters to define the number of poses and feature sample locations the camera pose finder uses.
            </param>
            <returns>The CameraPoseFinder instance.</returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="cameraPoseFinderParameters"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="cameraPoseFinderParameters"/> parameter's 
            <c>featureSampleLocationsPerFrameCount</c>, <c>maxPoseHistoryCount</c> member is not a greater than 0, 
            or the <c>featureSampleLocationsPerFrameCount</c> member is greater than 1000.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown when the memory required for the camera pose finder processing could not be
            allocated.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the Kinect device is not
            connected or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinder.FusionCreateCameraPoseFinder(Microsoft.Kinect.Fusion.CameraPoseFinderParameters,System.Int32)">
            <summary>
            Initialize a Kinect Fusion cameraPoseFinder, with random number generator seed for feature
            locations and feature thresholds.
            </summary>
            <param name="cameraPoseFinderParameters">
            The parameters to define the number of poses and feature sample locations the camera pose finder uses.
            </param>
            <param name="randomFeatureLocationAndThresholdSeed">
            A seed to initialize the random number generator for feature locations and feature thresholds.
            </param>
            <returns>The CameraPoseFinder instance.</returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="cameraPoseFinderParameters"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="cameraPoseFinderParameters"/> parameter's 
            <c>featureSampleLocationsPerFrameCount</c>, <c>maxPoseHistoryCount</c> member is not a greater than 0, 
            or a maximum of 10,000,000, the <c>featureSampleLocationsPerFrameCount</c> member is greater than 1000,
            or the <paramref name="randomFeatureLocationAndThresholdSeed"/> parameter is negative.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown when the memory required for the camera pose finder processing could not be
            allocated.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the Kinect device is not
            connected or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinder.ResetCameraPoseFinder">
            <summary>
            Clear the cameraPoseFinder.
            </summary>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinder.ProcessFrame(Microsoft.Kinect.Fusion.FusionFloatImageFrame,Microsoft.Kinect.Fusion.FusionColorImageFrame,Microsoft.Kinect.Fusion.Matrix4,System.Single,System.Boolean@,System.Boolean@)">
            <summary>
            Test input camera frames against the camera pose finder database, adding frames to the 
            database if dis-similar enough to existing frames. Both input depth and color frames 
            must be identical sizes, a minimum size of 80x60, with valid camera parameters, and 
            captured at the same time.
            Note that once the database reaches its maximum initialized size, it will overwrite old
            pose information. Check the <pararmref name="pHistoryTrimmed"/> flag or the number of 
            poses in the database to determine whether the old poses are being overwritten.
            </summary>
            <param name="depthFloatFrame">The depth float frame to be processed.</param>
            <param name="colorFrame">The color frame to be processed.</param>
            <param name="worldToCameraTransform"> The current camera pose (usually the camera pose 
            result from the last AlignPointClouds or AlignDepthFloatToReconstruction).</param>
            <param name="minimumDistanceThreshold">A float distance threshold between 0 and 1.0f which
            regulates how close together poses are stored in the database. Input frames
            which have a minimum distance equal to or above this threshold when compared against the 
            database will be stored, as it indicates the input has become dis-similar to the existing 
            stored poses. Set to 0.0f to ignore and always add a pose when this function is called, 
            however in this case, unless there is an external test of distance, there is a risk this
            can lead to many duplicated poses.
            </param>
            <param name="addedPose">
            Set true when the input frame was added to the camera pose finder database.
            </param>
            <param name="trimmedHistory">
            Set true if the maxPoseHistoryCount was reached when the input frame is stored, so the
            oldest pose was overwritten in the camera pose finder database to store the latest pose.
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthFloatFrame"/> or <paramref name="colorFrame"/> 
            parameter is null. </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="depthFloatFrame"/> and <paramref name="colorFrame"/> 
            parameter is an incorrect or different image size, or their <c>CameraParameters</c>
            member is null or has incorrectly sized focal lengths, or the 
            <paramref name="minimumDistanceThreshold"/> parameter is less than 0 or greater 
            than 1.0f.</exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            or the call failed for an unknown reason.
            </exception>
            <remarks>
            The camera pose finder works by accumulating whether the values at each sample location pixel
            in a saved pose frame are less than or greater than a threshold which is randomly chosen 
            between minimum and maximum boundaries (e.g. for color this is 0-255). Given enough samples
            this represents a unique key frame signature that we can match against, as different poses
            will have different values for surfaces which are closer or further away, or different
            colors.
            Note that unlike depth, the robustness of finding a valid camera pose can have issues with
            ambient illumination levels in the color image. For best matching results, both the Kinect 
            camera and also the environment should have exactly the same configuration as when the 
            database key frame images were captured i.e. if you had a fixed exposure and custom white
            balance, this should again be set when testing the database later, otherwise the matching
            accuracy will be reduced. 
            To improve accuracy, it is possible to not just provide a red, green, blue input in the
            color image, but instead provide a different 3 channels of match data scaled 0-255. For
            example, to be more illumination independent, you could calculate hue and saturation, or
            convert RGB to to LAB and use the AB channels. Other measures such as texture response
            or corner response could additionally be computed and used in one or more of the channels.
            </remarks>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinder.FindCameraPose(Microsoft.Kinect.Fusion.FusionFloatImageFrame,Microsoft.Kinect.Fusion.FusionColorImageFrame)">
            <summary>
            Find the most similar camera poses to the current camera input by comparing against the
            camera pose finder database, and returning a set of similar camera poses. These poses 
            and similarity measurements are ordered in terms of decreasing similarity (i.e. the most 
            similar is first). Both input depth and color frames must be identical sizes, with valid 
            camera parameters and captured at the same time.
            </summary>
            <param name="depthFloatFrame">The depth float frame to be processed.</param>
            <param name="colorFrame">The color frame to be processed.</param>
            <returns>Returns the matched frames object created by the camera pose finder.</returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthFloatFrame"/> or <paramref name="colorFrame"/> 
            parameter is null. </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="depthFloatFrame"/> and  <paramref name="colorFrame"/> 
            parameter is an incorrect or different image size, or their <c>CameraParameters</c>
            member is null or has incorrectly sized focal lengths.</exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, 
            or the call failed for an unknown reason.
            </exception>
            <returns>Returns a set of matched frames/poses.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinder.LoadCameraPoseFinderDatabase(System.String)">
            <summary>
            Load a previously saved camera pose finder database from disk. 
            Note: All existing data is replaced on a successful load of the database.
            If the database is saved to disk alongside the reconstruction volume, when both are
            re-loaded, this potentially enables reconstruction and tracking to be re-started 
            and the reconstruction updated by running the camera pose finder.
            </summary>
            <param name="fileName">The filename of the database file to load.</param>
            <exception cref="T:System.ArgumentException">
            Thrown when <paramref name="fileName"/> is incorrect or the file was not found.</exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinder.SaveCameraPoseFinderDatabase(System.String)">
            <summary>
            Save the camera pose finder database to disk. 
            If the camera pose finder database is saved to disk alongside the reconstruction volume, 
            when both are re-loaded, this potentially enables reconstruction and tracking to be 
            re-started and the reconstruction updated by running the camera pose finder.
            </summary>
            <param name="fileName">The filename of the database file to save.</param>
            <exception cref="T:System.ArgumentException">
            Thrown when <paramref name="fileName"/> is incorrect or the path was not found.</exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected, 
            the device does not have enough space, or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinder.GetCameraPoseFinderParameters">
            <summary>
            Get the parameters used in the camera pose finder.
            </summary>
            <returns>Returns the parameters used in the camera pose finder.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, or
            the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinder.GetStoredPoseCount">
            <summary>
            Get the count of stored frames in the camera pose finder database.
            </summary>
            <returns>Returns the count of stored poses in the database.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, 
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinder.Dispose">
            <summary>
            Disposes the CameraPoseFinder.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.CameraPoseFinderParameters">
            <summary>
            This class is used to setup the volume parameters.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.DefaultFeatureSampleLocationsPerFrame">
            <summary>
            The default number of feature sample locations per frame.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.DefaultMaxPoseHistoryCount">
            <summary>
            The default maximum pose history.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.DefaultMaxDepthThreshold">
            <summary>
            The default maximum depth threshold.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.defaultRelocalizationParameters">
            <summary>
            The private member variable to cache the default camera parameters.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.lockObject">
            <summary>
            A lock object protecting the default parameter creation.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.#ctor(System.Int32,System.Int32,System.Single)">
            <summary>
            Initializes a new instance of the CameraPoseFinderParameters class.
            </summary>
            <param name="featureSampleLocationsPerFrameCount">Number of features to extract per frame. This must be greater
            than 0 and  a maximum of 1000.</param>
            <param name="maxPoseHistoryCount">Maximum size of the camera pose finder pose history database. 
            This must be greater than 0 and less than 10,000,000.</param>
            <param name="maxDepthThreshold">Maximum depth to be used when choosing a threshold value for each
            sample features. This should be greater than 0.4f and a maximum of the closest working distance
            you expect in your scenario (i.e. if all your reconstruction is at short range 0-2m, set 2.0f here).
            Note that with the there is a trade-off, as setting large distances may make the system less 
            discriminative, hence more features may be required to maintain matching performance.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.#ctor">
            <summary>
            Prevents a default instance of the CameraPoseFinderParameters class from being created.
            The declaration of the default constructor is used for marshaling operations.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.GetHashCode">
            <summary>
            Calculates the hash code of the CameraPoseFinderParameters.
            </summary>
            <returns>The hash code.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.Equals(Microsoft.Kinect.Fusion.CameraPoseFinderParameters)">
            <summary>
            Determines if the two objects are equal.
            </summary>
            <param name="other">The object to compare to.</param>
            <returns>This method returns true if they are equal and false otherwise.</returns>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.Defaults">
            <summary>
            Gets the default parameters.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.FeatureSampleLocationsPerFrame">
            <summary>
            Gets the number of locations to sample features per frame.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.MaxPoseHistory">
            <summary>
            Gets the maximum size of the camera pose finder pose history database.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.CameraPoseFinderParameters.MaxDepthThreshold">
            <summary>
            Gets the maximum depth when choosing a threshold value for each sample feature in a key frame.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.ColorMesh">
            <summary>
            The Mesh object is created when meshing a color Reconstruction volume. This provides access to the vertices,
            normals, triangle indexes and vertex colors of the mesh.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.ColorMesh.vertices">
            <summary>
            The vertices read only collection.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.ColorMesh.normals">
            <summary>
            The normals read only collection.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.ColorMesh.triangleIndexes">
            <summary>
            The triangle indexes read only collection.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.ColorMesh.colors">
            <summary>
            The colors read only collection.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.ColorMesh.mesh">
            <summary>
            The native INuiFusionColorMesh interface wrapper.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.ColorMesh.disposed">
            <summary>
            Track whether Dispose has been called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorMesh.#ctor(Microsoft.Kinect.Fusion.INuiFusionColorMesh)">
            <summary>
            Initializes a new instance of the ColorMesh class.
            </summary>
            <param name="mesh">The mesh interface to be encapsulated.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorMesh.Finalize">
            <summary>
            Finalizes an instance of the ColorMesh class.
            This destructor will run only if the Dispose method does not get called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorMesh.GetVertices">
            <summary>
            Gets the collection of vertices. Each vertex has a corresponding normal with the same index.
            </summary>
            <returns>Returns a reference to the read only collection of the vertices.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorMesh.GetNormals">
            <summary>
            Gets the collection of normals. Each normal has a corresponding vertex with the same index.
            </summary>
            <returns>Returns a reference to the read only collection of the normals.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorMesh.GetTriangleIndexes">
            <summary>
            Gets the collection of triangle indexes. There are 3 indexes per triangle.
            </summary>
            <returns>Returns a reference to the read only collection of the triangle indexes.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorMesh.GetColors">
            <summary>
            Gets the collection of colors. There is one color per-vertex. Each color has a corresponding
            vertex with the same index.
            </summary>
            <returns>Returns a reference to the read only collection of the colors.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorMesh.Dispose">
            <summary>
            Disposes the Mesh.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorMesh.Dispose(System.Boolean)">
            <summary>
            Dispose of the native Mesh object.
            </summary>
            <param name="disposing">Whether the function was called from Dispose.</param>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.ColorReconstruction">
            <summary>
            Reconstruction encapsulates reconstruction volume creation updating and meshing functions with color.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.ColorReconstruction.volume">
            <summary>
            The native reconstruction interface wrapper.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.ColorReconstruction.disposed">
            <summary>
            Track whether Dispose has been called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.#ctor(Microsoft.Kinect.Fusion.INuiFusionColorReconstruction)">
            <summary>
            Initializes a new instance of the ColorReconstruction class.
            Default constructor used to initialize with the native color Reconstruction volume object.
            </summary>
            <param name="volume">
            The native Reconstruction volume object to be encapsulated.
            </param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.Finalize">
            <summary>
            Finalizes an instance of the ColorReconstruction class.
            This destructor will run only if the Dispose method does not get called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.FusionCreateReconstruction(Microsoft.Kinect.Fusion.ReconstructionParameters,Microsoft.Kinect.Fusion.ReconstructionProcessor,System.Int32,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Initialize a Kinect Fusion 3D Reconstruction Volume enabling use with color.
            Voxel volume axis sizes must be greater than 0 and a multiple of 32. A Kinect camera 
            is also required to be connected.
            </summary>
            <param name="reconstructionParameters">
            The Reconstruction parameters to define the size and shape of the reconstruction volume.
            </param>
            <param name="reconstructionProcessorType">
            the processor type to be used for all calls to the reconstruction volume object returned
            from this function.
            </param>
            <param name="deviceIndex">Set this variable to an explicit zero-based device index to use
            a specific GPU as enumerated by FusionDepthProcessor.GetDeviceInfo, or set to -1 to 
            automatically select the default device for a given processor type.
            </param>
            <param name="initialWorldToCameraTransform">
            The initial camera pose of the reconstruction volume with respect to the world origin. 
            Pass identity as the default camera pose. 
            </param>
            <returns>The Reconstruction instance.</returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="reconstructionParameters"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="reconstructionParameters"/> parameter's <c>VoxelX</c>,
            <c>VoxelY</c>, or <c>VoxelZ</c> member is not a greater than 0 and multiple of 32.
            Thrown when the <paramref name="deviceIndex"/> parameter is less than -1 or greater 
            than the number of available devices for the respective processor type.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown when the memory required for the Reconstruction volume processing could not be
            allocated.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the Kinect device is not
            connected or the Reconstruction volume is too big so a GPU memory allocation failed, 
            or the call failed for an unknown reason.
            </exception>
            <remarks>
            Users can select which device the processing is performed on with
            the <paramref name="reconstructionProcessorType"/> parameter. For those with multiple GPUs
            the <paramref name="deviceIndex"/> parameter also enables users to explicitly configure
            on which device the reconstruction volume is created.
            Note that this function creates a default world-volume transform. To set a non-default
            transform call ResetReconstruction with an appropriate Matrix4. This default transformation
            is a combination of translation in X,Y to locate the world origin at the center of the front
            face of the reconstruction volume cube, and scaling by the voxelsPerMeter reconstruction
            parameter to convert from the world coordinate system to volume voxel indices.
            </remarks>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.ResetReconstruction(Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Clear the volume, and set a new world-to-camera transform (camera view pose) or identity. 
            This internally sets the default world-to-volume transform. where the Kinect camera is
            translated in X,Y to the center of the front face of the volume cube, looking into the cube, 
            and the world coordinates are scaled to volume indices according to the voxels per meter 
            setting.
            </summary>
            <param name="initialWorldToCameraTransform">
            The initial camera pose with respect to the world origin. 
            Pass identity as the default camera pose. 
            </param>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.ResetReconstruction(Microsoft.Kinect.Fusion.Matrix4,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Clear the reconstruction volume, and set a world-to-camera transform (camera view pose)
            and a world-to-volume transform.
            The world-volume transform expresses the location and orientation of the world coordinate 
            system origin in volume coordinates and the scaling of the world coordinates to
            volume indices. In practice, this controls where the reconstruction volume appears in the
            real world with respect to the world origin position, or with respect to the camera if 
            identity is passed for the initial world-to-camera transform (as the camera and world 
            origins then coincide).
            To create your own world-volume transformation first get the current transform by calling
            GetCurrentWorldToVolumeTransform then either modify the matrix directly or multiply
            with your own similarity matrix to alter the volume translation or rotation with respect
            to the world coordinate system. Note that other transforms such as skew are not supported.
            To reset the volume while keeping the same world-volume transform, first get the current
            transform by calling GetCurrentWorldToVolumeTransform and pass this Matrix4 as the
            <paramref name="worldToVolumeTransform"/> parameter when calling this reset
            function.
            </summary>
            <param name="initialWorldToCameraTransform">
            The initial camera pose with respect to the world origin. 
            Pass identity as the default camera pose. 
            </param>
            <param name="worldToVolumeTransform">A  Matrix4 instance, containing the world to volume
            transform.
            </param>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.AlignDepthFloatToReconstruction(Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Int32,Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Single@,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Aligns a depth float image to the Reconstruction volume to calculate the new camera pose.
            This camera tracking method requires a Reconstruction volume, and updates the internal 
            camera pose if successful. The maximum image resolution supported in this function is 640x480.
            </summary>
            <param name="depthFloatFrame">The depth float frame to be processed.</param>
            <param name="maxAlignIterationCount">
            The maximum number of iterations of the algorithm to run. 
            The minimum value is 1. Using only a small number of iterations will have a faster runtime,
            however, the algorithm may not converge to the correct transformation.
            </param>
            <param name="deltaFromReferenceFrame">
            Optionally, a pre-allocated float image frame, to be filled with information about how
            well each observed pixel aligns with the passed in reference frame. This may be processed
            to create a color rendering, or may be used as input to additional vision algorithms such 
            as object segmentation. These residual values are normalized -1 to 1 and represent the 
            alignment cost/energy for each pixel. Larger magnitude values (either positive or negative)
            represent more discrepancy, and lower values represent less discrepancy or less information
            at that pixel.
            Note that if valid depth exists, but no reconstruction model exists behind the depth   
            pixels, 0 values indicating perfect alignment will be returned for that area. In contrast,
            where no valid depth occurs 1 values will always be returned. Pass null if not required.
            </param>
            <param name="alignmentEnergy">
            A float to receive a value describing how well the observed frame aligns to the model with
            the calculated pose. A larger magnitude value represent more discrepancy, and a lower value
            represent less discrepancy. Note that it is unlikely an exact 0 (perfect alignment) value 
            will ever be returned as every frame from the sensor will contain some sensor noise.
            </param>
            <param name="worldToCameraTransform">
            The best guess of the camera pose (usually the camera pose result from the last
            AlignPointClouds or AlignDepthFloatToReconstruction).
            </param>
            <returns>
            Returns true if successful; return false if the algorithm encountered a problem aligning
            the input depth image and could not calculate a valid transformation.
            </returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="depthFloatFrame"/> parameter is an incorrect image size.
            Thrown when the <paramref name="maxAlignIterationCount"/> parameter is less than 1 or
            an incorrect value.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
            <remarks>
            Note that this function is designed primarily for tracking either with static scenes when
            performing environment reconstruction, or objects which move rigidly when performing object
            reconstruction from a static camera. Consider using the function AlignPointClouds instead 
            if tracking failures occur due to parts of a scene which move non-rigidly or should be 
            considered as outliers, although in practice, such issues are best avoided by carefully 
            designing or constraining usage scenarios wherever possible.
            </remarks> 
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.GetCurrentWorldToCameraTransform">
            <summary>
            Get current internal world-to-camera transform (camera view pose).
            </summary>
            <returns>The current world to camera pose.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.GetCurrentWorldToVolumeTransform">
            <summary>
            Get current internal world-to-volume transform.
            Note: A right handed coordinate system is used, with the origin of the volume (i.e. voxel 0,0,0)
            at the top left of the front plane of the cube. Similar to bitmap images with top left origin, 
            +X is to the right, +Y down, and +Z is now forward from origin into the reconstruction volume.
            The default transform is a combination of translation in X,Y to locate the world origin at the
            center of the front face of the reconstruction volume cube (with the camera looking onto the
            volume along +Z), and scaling by the voxelsPerMeter reconstruction parameter to convert from
            world coordinate system to volume voxel indices.
            </summary>
            <returns>The current world to volume transform. This is a similarity transformation
            that converts world coordinates to volume coordinates.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.IntegrateFrame(Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Int32,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Integrates depth float data into the reconstruction volume from the passed
            camera pose.
            Note: this function will also set the internal camera pose.
            </summary>
            <param name="depthFloatFrame">The depth float frame to be integrated.</param>
            <param name="maxIntegrationWeight">
            A parameter to control the temporal smoothing of depth integration. Minimum value is 1.
            Lower values have more noisy representations, but objects that move integrate and 
            disintegrate faster, so are suitable for more dynamic environments. Higher values
            integrate objects more slowly, but provides finer detail with less noise.</param>
            <param name="worldToCameraTransform">
            The camera pose (usually the camera pose result from the last
            FusionDepthProcessor.AlignPointClouds or AlignDepthFloatToReconstruction).
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="maxIntegrationWeight"/> parameter is less than 1 or
            greater than the maximum unsigned short value.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.IntegrateFrame(Microsoft.Kinect.Fusion.FusionFloatImageFrame,Microsoft.Kinect.Fusion.FusionColorImageFrame,System.Int32,System.Single,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Integrates depth float data and color data into the reconstruction volume from the
            passed camera pose. Here the angle parameter constrains the integration to
            integrate color over a given angle relative to the surface normal (recommended use 
            is for thin structure scanning).
            </summary>
            <param name="depthFloatFrame">The depth float frame to be integrated.</param>
            <param name="colorFrame">The color frame to be integrated.</param>
            <param name="maxIntegrationWeight">
            A parameter to control the temporal smoothing of depth integration. Minimum value is 1.
            Lower values have more noisy representations, but objects that move integrate and 
            disintegrate faster, so are suitable for more dynamic environments. Higher values
            integrate objects more slowly, but provides finer detail with less noise.</param>
            <param name="maxColorIntegrationAngle">An angle parameter in degrees to specify the angle
            with respect to the surface normal over which color will be integrated. This can be used so
            only when the camera sensor is near parallel with the surface (i.e. the camera direction of
            view is perpendicular to the surface), or  +/- an angle from the surface normal direction that
            color is integrated. 
            Pass FusionDepthProcessor.DefaultColorIntegrationOfAllAngles to ignore and accept color from
            all angles (default, fastest processing).
            This angle relative to this normal direction vector describe the acceptance half angle, for 
            example, a +/- 90 degree acceptance angle in all directions (i.e. a 180 degree hemisphere) 
            relative to the normal would integrate color in any orientation of the sensor towards the 
            front of the surface, even when parallel to the surface, whereas a 0 acceptance angle would
            only integrate color directly along a single ray exactly perpendicular to the surface.
            In reality, the useful range of values is actually between 0 and 90 exclusively 
            (e.g. setting +/- 60 degrees = 120 degrees total acceptance angle).
            Note that there is a trade-off here, as setting this has a runtime cost, however, conversely,
            ignoring this will integrate color from any angle over all voxels along camera rays around the
            zero crossing surface region in the volume, which can cause thin structures to have the same 
            color on both sides</param>
            <param name="worldToCameraTransform">
            The camera pose (usually the camera pose result from the last AlignPointClouds or 
            AlignDepthFloatToReconstruction).
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthFloatFrame"/> or <paramref name="colorFrame"/> 
            parameter is null.</exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="maxIntegrationWeight"/> parameter is less than 1 or
            greater than the maximum unsigned short value, or the 
            Thrown when the <paramref name="maxColorIntegrationAngle"/> parameter value is not
            FusionDepthProcessor.DefaultColorIntegrationOfAllAngles or between 0 and 90 degrees,
            exclusively.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.ProcessFrame(Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Int32,System.Int32,System.Single@,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            A high-level function to process a depth frame through the Kinect Fusion pipeline.
            Specifically, this performs processing equivalent to the following functions for each frame:
            <para>
            1) AlignDepthFloatToReconstruction
            2) IntegrateFrame
            </para>
            If there is a tracking error in the AlignDepthFloatToReconstruction stage, no depth data 
            integration will be performed, and the camera pose will remain unchanged.
            The maximum image resolution supported in this function is 640x480.
            </summary>
            <param name="depthFloatFrame">The depth float frame to be processed.</param>
            <param name="maxAlignIterationCount">
            The maximum number of iterations of the align camera tracking algorithm to run.
            The minimum value is 1. Using only a small number of iterations will have a faster
            runtime, however, the algorithm may not converge to the correct transformation.
            </param>
            <param name="maxIntegrationWeight">
            A parameter to control the temporal smoothing of depth integration. Lower values have
            more noisy representations, but objects that move appear and disappear faster, so are
            suitable for more dynamic environments. Higher values integrate objects more slowly,
            but provides finer detail with less noise.
            </param>
            <param name="alignmentEnergy">
            A float to receive a value describing how well the observed frame aligns to the model with
            the calculated pose. A larger magnitude value represent more discrepancy, and a lower value
            represent less discrepancy. Note that it is unlikely an exact 0 (perfect alignment) value 
            will ever be returned as every frame from the sensor will contain some sensor noise.
            </param>
            <param name="worldToCameraTransform">
            The best guess of the latest camera pose (usually the camera pose result from the last
            process call).
            </param>
            <returns>
            Returns true if successful; return false if the algorithm encountered a problem aligning
            the input depth image and could not calculate a valid transformation.
            </returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="depthFloatFrame"/> parameter is an incorrect image size.
            Thrown when the <paramref name="maxAlignIterationCount"/> parameter is less than 1 or
            greater than the maximum unsigned short value.
            Thrown when the <paramref name="maxIntegrationWeight"/> parameter is less than 1 or 
            greater than the maximum unsigned short value.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            or the call failed for an unknown reason.
            </exception>
            <remarks>
            Users may also optionally call the low-level functions individually, instead of calling this
            function, for more control. However, this function call will be faster due to the integrated 
            nature of the calls. After this call completes, if a visible output image of the reconstruction
            is required, the user can call CalculatePointCloud and then FusionDepthProcessor.ShadePointCloud.
            </remarks>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.ProcessFrame(Microsoft.Kinect.Fusion.FusionFloatImageFrame,Microsoft.Kinect.Fusion.FusionColorImageFrame,System.Int32,System.Int32,System.Single,System.Single@,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            A high-level function to process a depth frame through the Kinect Fusion pipeline.
            Also integrates color, further using a parameter to constrain the integration to
            integrate color over a given angle relative to the surface normal (recommended use 
            is for thin structure scanning).
            Specifically, this performs processing equivalent to the following functions for each frame:
            <para>
            1) AlignDepthFloatToReconstruction
            2) IntegrateFrame
            </para>
            If there is a tracking error in the AlignDepthFloatToReconstruction stage, no depth data 
            integration will be performed, and the camera pose will remain unchanged.
            The maximum image resolution supported in this function is 640x480.
            </summary>
            <param name="depthFloatFrame">The depth float frame to be processed.</param>
            <param name="colorFrame">The color frame to be processed.</param>
            <param name="maxAlignIterationCount">
            The maximum number of iterations of the align camera tracking algorithm to run.
            The minimum value is 1. Using only a small number of iterations will have a faster
            runtime, however, the algorithm may not converge to the correct transformation.
            </param>
            <param name="maxIntegrationWeight">
            A parameter to control the temporal smoothing of depth integration. Lower values have
            more noisy representations, but objects that move appear and disappear faster, so are
            suitable for more dynamic environments. Higher values integrate objects more slowly,
            but provides finer detail with less noise.
            </param>
            <param name="maxColorIntegrationAngle">An angle parameter in degrees to specify the angle
            with respect to the surface normal over which color will be integrated.This can be used so
            only when the camera sensor is near parallel with the surface (i.e. the camera direction of
            view is perpendicular to the surface), or  +/- an angle from the surface normal direction that
            color is integrated. 
            Pass FusionDepthProcessor.DefaultColorIntegrationOfAllAngles to ignore and accept color from
            all angles (default, fastest processing).
            This angle relative to this normal direction vector describe the acceptance half angle, for 
            example, a +/- 90 degree acceptance angle in all directions (i.e. a 180 degree hemisphere) 
            relative to the normal would integrate color in any orientation of the sensor towards the 
            front of the surface, even when parallel to the surface, whereas a 0 acceptance angle would
            only integrate color directly along a single ray exactly perpendicular to the surface.
            In reality, the useful range of values is actually between 0 and 90 exclusively 
            (e.g. setting +/- 60 degrees = 120 degrees total acceptance angle).
            Note that there is a trade-off here, as setting this has a runtime cost, however, conversely,
            ignoring this will integrate color from any angle over all voxels along camera rays around the
            zero crossing surface region in the volume, which can cause thin structures to have the same 
            color on both sides.</param>
            <param name="alignmentEnergy">
            A float to receive a value describing how well the observed frame aligns to the model with
            the calculated pose. A larger magnitude value represent more discrepancy, and a lower value
            represent less discrepancy. Note that it is unlikely an exact 0 (perfect alignment) value 
            will ever be returned as every frame from the sensor will contain some sensor noise.
            </param>
            <param name="worldToCameraTransform">
            The best guess of the latest camera pose (usually the camera pose result from the last
            process call).
            </param>
            <returns>
            Returns true if successful; return false if the algorithm encountered a problem aligning
            the input depth image and could not calculate a valid transformation.
            </returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthFloatFrame"/> or <paramref name="colorFrame"/>  
            parameter is null. </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="depthFloatFrame"/> or <paramref name="colorFrame"/>  
            parameter is an incorrect image size.
            Thrown when the <paramref name="maxAlignIterationCount"/> parameter is less than 1 or
            greater than the maximum unsigned short value.
            Thrown when the <paramref name="maxIntegrationWeight"/> parameter is less than 1 or 
            greater than the maximum unsigned short value.
            Thrown when the <paramref name="maxColorIntegrationAngle"/> parameter value is not
            FusionDepthProcessor.DefaultColorIntegrationOfAllAngles or between 0 and 90 degrees, 
            exclusively.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            or the call failed for an unknown reason.
            </exception>
            <remarks>
            Users may also optionally call the low-level functions individually, instead of calling this
            function, for more control. However, this function call will be faster due to the integrated 
            nature of the calls. After this call completes, if a visible output image of the reconstruction
            is required, the user can call CalculatePointCloud and then FusionDepthProcessor.ShadePointCloud.
            </remarks>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.CalculatePointCloud(Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Calculate a point cloud by raycasting into the reconstruction volume, returning the point
            cloud containing 3D points and normals of the zero-crossing dense surface at every visible
            pixel in the image from the given camera pose.
            This point cloud can be used as a reference frame in the next call to
            FusionDepthProcessor.AlignPointClouds, or passed to FusionDepthProcessor.ShadePointCloud
            to produce a visible image output.
            The <paramref name="pointCloudFrame"/> can be an arbitrary image size, for example, enabling
            you to calculate point clouds at the size of your window and then create a visible image by
            calling FusionDepthProcessor.ShadePointCloud and render this image, however, be aware that 
            large images will be expensive to calculate.
            </summary>
            <param name="pointCloudFrame">
            The pre-allocated point cloud frame, to be filled by raycasting into the reconstruction volume.
            Typically used as the reference frame with the FusionDepthProcessor.AlignPointClouds function
            or for visualization by calling FusionDepthProcessor.ShadePointCloud.
            </param>
            <param name="worldToCameraTransform">
            The world to camera transform (camera pose) to raycast from.
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="pointCloudFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.CalculatePointCloud(Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,Microsoft.Kinect.Fusion.FusionColorImageFrame,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Calculate a point cloud by raycasting into the reconstruction volume, returning the point
            cloud containing 3D points and normals of the zero-crossing dense surface at every visible
            pixel in the image from the given camera pose, and optionally the color visualization image.
            This point cloud can be used as a reference frame in the next call to
            FusionDepthProcessor.AlignPointClouds, or passed to FusionDepthProcessor.ShadePointCloud
            to produce a visible image output.
            The <paramref name="pointCloudFrame"/> can be an arbitrary image size, for example, enabling
            you to calculate point clouds at the size of your window and then create a visible image by
            calling FusionDepthProcessor.ShadePointCloud and render this image, however, be aware that 
            large images will be expensive to calculate.
            </summary>
            <param name="pointCloudFrame">
            The pre-allocated point cloud frame, to be filled by raycasting into the reconstruction volume.
            Typically used as the reference frame with the FusionDepthProcessor.AlignPointClouds function
            or for visualization by calling FusionDepthProcessor.ShadePointCloud.
            </param>
            <param name="colorFrame">Optionally, the color frame to fill. Pass null to ignore.</param>
            <param name="worldToCameraTransform">
            The world to camera transform (camera pose) to raycast from.
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="pointCloudFrame"/> parameter is null. </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.CalculateMesh(System.Int32)">
            <summary>
            Export a polygon mesh of the zero-crossing dense surfaces from the reconstruction volume
            with per-vertex color.
            </summary>
            <param name="voxelStep">
            The step value in voxels for sampling points to use in the volume when exporting a mesh, which
            determines the final resolution of the mesh. Use higher values for lower resolution meshes. 
            voxelStep must be greater than 0 and smaller than the smallest volume axis voxel resolution. 
            To mesh the volume at its full resolution, use a step value of 1.
            Note: Any value higher than 1 for this parameter runs the risk of missing zero crossings, and
            hence missing surfaces or surface details.
            </param>
            <returns>Returns the mesh object created by Kinect Fusion.</returns>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="voxelStep"/> parameter is less than 1 or 
            greater than the maximum unsigned int value or the smallest volume axis resolution.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if the CPU memory required for mesh calculation could not be allocated.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.ExportVolumeBlock(System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int16[])">
            <summary>
            Export a part or all of the reconstruction volume as a short array. 
            The surface boundary occurs where the tri-linearly interpolated voxel values have a zero crossing
            (i.e. when an interpolation crosses from positive to negative or vice versa). A voxel value of 
            0x8000 indicates that a voxel is uninitialized and has no valid data associated with it.
            </summary>
            <param name="sourceOriginX">The reconstruction volume voxel index in the X axis from which the 
            extraction should begin. This value must be greater than or equal to 0 and less than the
            reconstruction volume X axis voxel resolution.</param>
            <param name="sourceOriginY">The reconstruction volume voxel index in the Y axis from which the 
            extraction should begin. This value must be greater than or equal to 0 and less than the
            reconstruction volume Y axis voxel resolution.</param>
            <param name="sourceOriginZ">The reconstruction volume voxel index in the Z axis from which the 
            extraction should begin. This value must be greater than or equal to 0 and less than the
            reconstruction volume Z axis voxel resolution.</param>
            <param name="destinationResolutionX">The X axis resolution/width of the new voxel volume to return
            in the array. This value must be greater than 0 and less than or equal to the current volume X 
            axis voxel resolution. The final count of (sourceOriginX+(destinationResolutionX*voxelStep) must 
            not be greater than the current reconstruction volume X axis voxel resolution.</param>
            <param name="destinationResolutionY">The Y axis resolution/height of the new voxel volume to return
            in the array. This value must be greater than 0 and less than or equal to the current volume Y 
            axis voxel resolution. The final count of (sourceOriginY+(destinationResolutionY*voxelStep) must 
            not be greater than the current reconstruction volume Y axis voxel resolution.</param>
            <param name="destinationResolutionZ">The Z axis resolution/depth of the new voxel volume to return
            in the array. This value must be greater than 0 and less than or equal to the current volume Z 
            axis voxel resolution. The final count of (sourceOriginZ+(destinationResolutionZ*voxelStep) must 
            not be greater than the current reconstruction volume Z axis voxel resolution.</param>
            <param name="voxelStep">The step value in integer voxels for sampling points to use in the
            volume when exporting. The value must be greater than 0 and less than the smallest 
            volume axis voxel resolution. To export the volume at its full resolution, use a step value of 1. 
            Use higher step values to skip voxels and return the new volume as if there were a lower effective 
            resolution volume. For example, when exporting with a destination resolution of 320^3, setting 
            voxelStep to 2 would actually cover a 640^3 voxel are a(destinationResolution*voxelStep) in the 
            source reconstruction, but the data returned would skip every other voxel in the original volume.
            NOTE:  Any value higher than 1 for this value runs the risk of missing zero crossings, and hence
            missing surfaces or surface details.</param>
            <param name="volumeBlock">A pre-allocated short array to be filled with 
            volume data. The number of elements in this user array should be allocated as:
            (destinationResolutionX * destinationResolutionY * destinationResolutionZ) 
            To access the voxel located at x,y,z use pVolume[z][y][x], or index as 1D array for a particular
            voxel(x,y,z) as follows: with pitch = x resolution, slice = (y resolution * pitch)
            unsigned int index = (z * slice)  + (y * pitch) + x;
            Note: A right handed coordinate system is used, with the origin of the volume (i.e. voxel 0,0,0) 
            at the top left of the front plane of the cube. Similar to bitmap images with top left origin, 
            +X is to the right, +Y down, and +Z is forward from origin into the reconstruction volume.
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="volumeBlock"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="volumeBlock"/> parameter length is not equal to
            (<paramref name="destinationResolutionX"/> * <paramref name="destinationResolutionY"/> *
            <paramref name="destinationResolutionZ"/>).
            Thrown when a sourceOrigin or destinationResolution parameter less than 1 or
            greater than the maximum unsigned short value.
            Thrown when the (sourceOrigin+(destinationResolution*voxelStep) calculation was
            greater than the current reconstruction volume voxel resolution along an axis.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if the CPU memory required for volume export could not be allocated.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.ExportVolumeBlock(System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int16[],System.Int32[])">
            <summary>
            Export a part or all of the reconstruction volume as a short array with color as int array. 
            The surface boundary occurs where the tri-linearly interpolated voxel values have a zero crossing
            (i.e. when an interpolation crosses from positive to negative or vice versa). A voxel value of 
            0x8000 indicates that a voxel is uninitialized and has no valid data associated with it.
            </summary>
            <param name="sourceOriginX">The reconstruction volume voxel index in the X axis from which the 
            extraction should begin. This value must be greater than or equal to 0 and less than the
            reconstruction volume X axis voxel resolution.</param>
            <param name="sourceOriginY">The reconstruction volume voxel index in the Y axis from which the 
            extraction should begin. This value must be greater than or equal to 0 and less than the
            reconstruction volume Y axis voxel resolution.</param>
            <param name="sourceOriginZ">The reconstruction volume voxel index in the Z axis from which the 
            extraction should begin. This value must be greater than or equal to 0 and less than the
            reconstruction volume Z axis voxel resolution.</param>
            <param name="destinationResolutionX">The X axis resolution/width of the new voxel volume to return
            in the array. This value must be greater than 0 and less than or equal to the current volume X 
            axis voxel resolution. The final count of (sourceOriginX+(destinationResolutionX*voxelStep) must 
            not be greater than the current reconstruction volume X axis voxel resolution.</param>
            <param name="destinationResolutionY">The Y axis resolution/height of the new voxel volume to return
            in the array. This value must be greater than 0 and less than or equal to the current volume Y 
            axis voxel resolution. The final count of (sourceOriginY+(destinationResolutionY*voxelStep) must 
            not be greater than the current reconstruction volume Y axis voxel resolution.</param>
            <param name="destinationResolutionZ">The Z axis resolution/depth of the new voxel volume to return
            in the array. This value must be greater than 0 and less than or equal to the current volume Z 
            axis voxel resolution. The final count of (sourceOriginZ+(destinationResolutionZ*voxelStep) must 
            not be greater than the current reconstruction volume Z axis voxel resolution.</param>
            <param name="voxelStep">The step value in integer voxels for sampling points to use in the
            volume when exporting. The value must be greater than 0 and less than the smallest 
            volume axis voxel resolution. To export the volume at its full resolution, use a step value of 1. 
            Use higher step values to skip voxels and return the new volume as if there were a lower effective 
            resolution volume. For example, when exporting with a destination resolution of 320^3, setting 
            voxelStep to 2 would actually cover a 640^3 voxel are a(destinationResolution*voxelStep) in the 
            source reconstruction, but the data returned would skip every other voxel in the original volume.
            NOTE:  Any value higher than 1 for this value runs the risk of missing zero crossings, and hence
            missing surfaces or surface details.</param>
            <param name="volumeBlock">A pre-allocated short array to be filled with 
            volume data. The number of elements in this user array should be allocated as:
            (destinationResolutionX * destinationResolutionY * destinationResolutionZ) 
            To access the voxel located at x,y,z use pVolume[z][y][x], or index as 1D array for a particular
            voxel(x,y,z) as follows: with pitch = x resolution, slice = (y resolution * pitch)
            unsigned int index = (z * slice)  + (y * pitch) + x;
            Note: A right handed coordinate system is used, with the origin of the volume (i.e. voxel 0,0,0) 
            at the top left of the front plane of the cube. Similar to bitmap images with top left origin, 
            +X is to the right, +Y down, and +Z is forward from origin into the reconstruction volume.
            </param>
            <param name="colorVolumeBlock">A pre-allocated int array filled with color volume data.
            The number of elements must be identical to those in <paramref name="volumeBlock"/>. 
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="volumeBlock"/> or <paramref name="colorVolumeBlock"/> parameter 
            is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="volumeBlock"/> or <paramref name="colorVolumeBlock"/> parameter
            length is not equal to:
            (<paramref name="destinationResolutionX"/> * <paramref name="destinationResolutionY"/> *
            <paramref name="destinationResolutionZ"/>).
            Thrown when a sourceOrigin or destinationResolution parameter less than 1 or
            greater than the maximum unsigned short value.
            Thrown when the (sourceOrigin+(destinationResolution*voxelStep) calculation was
            greater than the current reconstruction volume voxel resolution along an axis.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if the CPU memory required for volume export could not be allocated.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.ImportVolumeBlock(System.Int16[])">
            <summary>
            Import a reconstruction volume as a short array. 
            This array must equal the size of the current initialized reconstruction volume.
            </summary>
            <param name="volumeBlock">A pre-allocated short array filled with volume data.
            The number of elements in this user array should be allocated as:
            (sourceResolutionX * sourceResolutionY * sourceResolutionZ) 
            To access the voxel located at x,y,z use pVolume[z][y][x], or index as 1D array for a particular
            voxel(x,y,z) as follows: with pitch = x resolution, slice = (y resolution * pitch)
            unsigned int index = (z * slice)  + (y * pitch) + x;
            Note: A right handed coordinate system is used, with the origin of the volume (i.e. voxel 0,0,0) 
            at the top left of the front plane of the cube. Similar to bitmap images with top left origin, 
            +X is to the right, +Y down, and +Z is forward from origin into the reconstruction volume.
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="volumeBlock"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="volumeBlock"/> parameter length is not equal to
            the existing initialized volume.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if the CPU memory required for volume export could not be allocated.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.ImportVolumeBlock(System.Int16[],System.Int32[])">
            <summary>
            Import a reconstruction volume as a short array with color as int array. 
            This arrays must both have the same number of elements and this must be equal to the size of the
            current initialized reconstruction volume.
            </summary>
            <param name="volumeBlock">A pre-allocated short array filled with volume data.
            The number of elements in this user array should be allocated as:
            (sourceResolutionX * sourceResolutionY * sourceResolutionZ) 
            To access the voxel located at x,y,z use pVolume[z][y][x], or index as 1D array for a particular
            voxel(x,y,z) as follows: with pitch = x resolution, slice = (y resolution * pitch)
            unsigned int index = (z * slice)  + (y * pitch) + x;
            Note: A right handed coordinate system is used, with the origin of the volume (i.e. voxel 0,0,0) 
            at the top left of the front plane of the cube. Similar to bitmap images with top left origin, 
            +X is to the right, +Y down, and +Z is forward from origin into the reconstruction volume.
            </param>
            <param name="colorVolumeBlock">A pre-allocated int array filled with color volume data.
            The number of elements must be identical to those in <paramref name="volumeBlock"/>. 
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="volumeBlock"/> or <paramref name="colorVolumeBlock"/> parameter
             is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="volumeBlock"/> or <paramref name="colorVolumeBlock"/> parameter 
            length is not equal to the existing initialized volume or the two array lengths are different.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if the CPU memory required for volume export could not be allocated.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.DepthToDepthFloatFrame(System.UInt16[],Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Single,System.Single,System.Boolean)">
            <summary>
            Converts Kinect depth frames in unsigned short format to depth frames in float format 
            representing distance from the camera in meters (parallel to the optical center axis).
            Note: <paramref name="depthImageData"/> and <paramref name="depthFloatFrame"/> must
            be the same pixel resolution. This version of the function runs on the GPU.
            </summary>
            <param name="depthImageData">The source depth data.</param>
            <param name="depthFloatFrame">A depth float frame, to be filled with depth.</param>
            <param name="minDepthClip">The minimum depth threshold. Values below this will be set to 0.</param>
            <param name="maxDepthClip">The maximum depth threshold. Values above this will be set to 1000.</param>
            <param name="mirrorDepth">Set true to mirror depth, false so the image appears correct if viewing
            the Kinect camera from behind.</param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthImageData"/> or 
            <paramref name="depthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="depthImageData"/> or
            <paramref name="depthFloatFrame"/> parameter is an incorrect image size, or the 
            kernelWidth is an incorrect size.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
            <remarks>
            The min and max depth clip values enable clipping of the input data, for example, to help
            isolate particular objects or surfaces to be reconstructed. Note that the thresholds return 
            different values when a depth pixel is outside the threshold - pixels inside minDepthClip will
            will be returned as 0 and ignored in processing, whereas pixels beyond maxDepthClip will be set
            to 1000 to signify a valid depth ray with depth beyond the set threshold. Setting this far-
            distance flag is important for reconstruction integration in situations where the camera is
            static or does not move significantly, as it enables any voxels closer to the camera
            along this ray to be culled instead of persisting (as would happen if the pixels were simply 
            set to 0 and ignored in processing). Note that when reconstructing large real-world size volumes,
            be sure to set large maxDepthClip distances, as when the camera moves around, any voxels in view
            which go beyond this threshold distance from the camera will be removed.
            </remarks>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.SmoothDepthFloatFrame(Microsoft.Kinect.Fusion.FusionFloatImageFrame,Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Int32,System.Single)">
            <summary>
            Spatially smooth a depth float image frame using edge-preserving filtering on GPU. 
            </summary>
            <param name="depthFloatFrame">A source depth float frame.</param>
            <param name="smoothDepthFloatFrame">A depth float frame, to be filled with smoothed 
            depth.</param>
            <param name="kernelWidth">Smoothing Kernel Width. Valid values are  1,2,3 
            (for 3x3,5x5,7x7 smoothing kernel block size respectively).</param>
            <param name="distanceThreshold">A distance difference range that smoothing occurs in.
            Pixels with neighboring pixels outside this distance range will not be smoothed 
            (larger values indicate discontinuity/edge). Must be greater than 0.</param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="smoothDepthFloatFrame"/> or 
            <paramref name="depthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="smoothDepthFloatFrame"/> or
            <paramref name="depthFloatFrame"/> parameter is an incorrect image size, or the 
            kernelWidth is an incorrect size.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.AlignPointClouds(Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,System.Int32,Microsoft.Kinect.Fusion.FusionColorImageFrame,System.Single@,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            The AlignPointClouds function uses an on GPU iterative algorithm to align two sets of  
            overlapping oriented point clouds and calculate the camera's relative pose.
            All images must be the same size and have the same camera parameters. 
            </summary>
            <param name="referencePointCloudFrame">A reference point cloud frame.</param>
            <param name="observedPointCloudFrame">An observerd point cloud frame.</param>
            <param name="maxAlignIterationCount">The number of iterations to run.</param>
            <param name="deltaFromReferenceFrame">
            Optionally, a pre-allocated color image frame, to be filled with color-coded data
            from the camera tracking. This may be used as input to additional vision algorithms such as
            object segmentation. Values vary depending on whether the pixel was a valid pixel used in
            tracking (inlier) or failed in different tests (outlier). 0xff000000 indicates an invalid 
            input vertex (e.g. from 0 input depth), or one where no correspondences occur between point
            cloud images. Outlier vertices rejected due to too large a distance between vertices are 
            coded as 0xff008000. Outlier vertices rejected due to to large a difference in normal angle
            between point clouds are coded as 0xff800000. Inliers are color shaded depending on the 
            residual energy at that point, with more saturated colors indicating more discrepancy
            between vertices and less saturated colors (i.e. more white) representing less discrepancy,
            or less information at that pixel. Pass null if this image is not required.
            </param>
            <param name="alignmentEnergy">A value describing
            how well the observed frame aligns to the model with the calculated pose (mean distance between
            matching points in the point clouds). A larger magnitude value represent more discrepancy, and 
            a lower value represent less discrepancy. Note that it is unlikely an exact 0 (perfect alignment) 
            value will ever/ be returned as every frame from the sensor will contain some sensor noise. 
            Pass NULL to ignore this parameter.</param>
            <param name="referenceToObservedTransform">The initial guess at the transform. This is 
            updated on tracking success, or returned as identity on failure.</param>
            <returns>
            Returns true if successful; return false if the algorithm encountered a problem aligning
            the input depth image and could not calculate a valid transformation.
            </returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="referencePointCloudFrame"/> or 
            <paramref name="observedPointCloudFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="referencePointCloudFrame"/> or
            <paramref name="observedPointCloudFrame"/> or <paramref name="deltaFromReferenceFrame"/>
            parameter is an incorrect image size, or the iterations parameter is not greater than 0.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.SetAlignDepthFloatToReconstructionReferenceFrame(Microsoft.Kinect.Fusion.FusionFloatImageFrame)">
            <summary>
            Set a reference depth frame to be used internally to help with tracking when calling 
            AlignDepthFloatToReconstruction to calculate a new camera pose. This function should
            only be called when not using the default tracking behavior of Kinect Fusion.
            </summary>
            <remarks>
            AlignDepthFloatToReconstruction internally saves the last depth frame it was passed and
            uses this image to help it track when called the next time. For example, this can be used
            if you are reconstructing and lose track, then want to re-start tracking from a different
            (known) location without resetting the volume. To enable the tracking to succeed you
            could perform a raycast from the new location to get a depth image (by calling 
            CalculatePointCloudAndDepth) then call this set function with the depth image, before 
            calling AlignDepthFloatToReconstruction.
            </remarks>
            <param name="referenceDepthFloatFrame">A previous depth float frame where align was
            successful (and hence same functionality as AlignDepthFloatToReconstruction), 
            or a ray-casted model depth.</param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="referenceDepthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="referenceDepthFloatFrame"/> parameter is an incorrect 
            image size.</exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.CalculatePointCloudAndDepth(Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,Microsoft.Kinect.Fusion.FusionFloatImageFrame,Microsoft.Kinect.Fusion.FusionColorImageFrame,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Calculate a point cloud, depth and optionally color image by raycasting into the 
            reconstruction volume. This returns the point cloud containing 3D points and normals of the 
            zero-crossing dense surface at every visible pixel in the image from the given camera pose,
            the depth to the surface, and optionally the color visualization image.
            </summary>
            <param name="pointCloudFrame">A point cloud frame, to be filled by raycasting into the 
            reconstruction volume. Typically used as the reference frame with the 
            FusionDepthProcessor.AlignPointClouds function or for visualization by calling 
            FusionDepthProcessor.ShadePointCloud.</param>
            <param name="depthFloatFrame">A floating point depth frame, to be filled with floating point
            depth in meters to the raycast surface. This image must be identical in size, and camera 
            parameters to the <paramref name="pointCloudFrame"/> parameter.</param>
            <param name="colorFrame">Optionally, the color frame to fill. Pass null to ignore.</param>
            <param name="worldToCameraTransform">The world-to-camera transform (camera pose) to 
            raycast from.</param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="pointCloudFrame"/> or 
            <paramref name="depthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="pointCloudFrame"/> or <paramref name="depthFloatFrame"/> or
            <paramref name="colorFrame"/> parameter is an incorrect image size.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
            <remarks>
            This point cloud can then be used as a reference frame in the next call to 
            FusionDepthProcessor.AlignPointClouds, or passed to FusionDepthProcessor.ShadePointCloud
            to produce a visible image output.The depth image can be used as a reference frame for
            AlignDepthFloatToReconstruction by calling SetAlignDepthFloatToReconstructionReferenceFrame 
            to enable a greater range of tracking. The <paramref name="pointCloudFrame"/> and 
            <paramref name="depthFloatFrame"/> parameters can be an arbitrary image size, for example, 
            enabling you to calculate point clouds at the size of your UI window and then create a visible
            image by calling FusionDepthProcessor.ShadePointCloud and render this image, however, be aware
            that the calculation of high resolution images will be expensive in terms of runtime.
            </remarks>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.Dispose">
            <summary>
            Disposes the Reconstruction.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ColorReconstruction.Dispose(System.Boolean)">
            <summary>
            Frees all memory associated with the Reconstruction.
            </summary>
            <param name="disposing">Whether the function was called from Dispose.</param>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.MatchCandidates">
            <summary>
            The MatchCandidates object is created when finding a camera pose using the camera pose finder. 
            This provides access to the matched camera poses and their similarity measurements.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.MatchCandidates.poses">
            <summary>
            The poses read only collection.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.MatchCandidates.similarityMeasurements">
            <summary>
            The similarity measurements read only collection.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.MatchCandidates.matchCandidates">
            <summary>
            The native INuiFusionMatchCandidates interface wrapper.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.MatchCandidates.disposed">
            <summary>
            Track whether Dispose has been called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.MatchCandidates.#ctor(Microsoft.Kinect.Fusion.INuiFusionMatchCandidates)">
            <summary>
            Initializes a new instance of the MatchCandidates class.
            </summary>
            <param name="matchCandidates">The match candidates interface to be encapsulated.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.MatchCandidates.Finalize">
            <summary>
            Finalizes an instance of the MatchCandidates class.
            This destructor will run only if the Dispose method does not get called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.MatchCandidates.GetPoseCount">
            <summary>
            Gets the number of matched poses and size of the collection returned 
            by calling GetMatchPoses.
            </summary>
            <returns>Returns the count of smatched poses.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.MatchCandidates.GetMatchPoses">
            <summary>
            Gets a collection of camera pose candidates matched to the input in the camera pose finder. 
            The poses are sorted in terms of descending similarity (i.e. the most similar is first).
            Each pose has a corresponding similarity measurement with the same index.
            </summary>
            <returns>Returns a reference to the read only collection of the poses.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.MatchCandidates.GetSimilarityCount">
            <summary>
            Gets the number of similarity measurements and size of the collection returned 
            by calling GetSimilarityCount.
            </summary>
            <returns>Returns the count of similarity measurements.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.MatchCandidates.GetMatchSimilarities">
            <summary>
            Gets the collection of match similarity measurements indicating how similar the match
            is to the input to the camera pose finder.  The similarity measure is a count of how many
            features were matched, normalized to a value between 0 and 1.0f by dividing the 
            featureSampleLocationsPerFrameCount.
            Larger values indicate a better match and hence more similarity between the input and the 
            respective pose. Each similarity measure has a corresponding pose with the same index and and
            are sorted in terms of descending similarity (i.e. the most similar is first).
            </summary>
            <returns>Returns a reference to the read only collection of the 
            similarity measurements.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.MatchCandidates.CalculateMinimumDistance">
            <summary>
            Calculate the minimum distance between the input and the matches.
            (i.e. between the input and single most similar pose candidate).
            Minimum distance is returned normalized 0-1, with a smaller value indicating higher
            similarity and a larger value indicating less similarity. 
            You can regulate how close together poses are stored in the camera pose finder database by
            only calling ProcessFrame when this value goes above a certain threshold, indicating that 
            the input is becoming dissimilar to the existing stored poses. 
            </summary>
            <returns>Returns a float containing the minimum distance measure.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.MatchCandidates.Dispose">
            <summary>
            Disposes the match candidates.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.Matrix4">
            <summary>
            Matrix4 is a row-major matrix containing the joint rotation information
            in the top left 3x3 and zero for translation.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Matrix4.op_Equality(Microsoft.Kinect.Fusion.Matrix4,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Compares two Matrix4 objects for equality.
            </summary>
            <param name="mat1">The first Matrix4 to compare.</param>
            <param name="mat2">The second Matrix4 to compare.</param>
            <returns>Returns true if they are equal and false otherwise.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Matrix4.op_Inequality(Microsoft.Kinect.Fusion.Matrix4,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Compares two Matrix4 objects for inequality.
            </summary>
            <param name="mat1">The first Matrix4 to compare.</param>
            <param name="mat2">The second Matrix4 to compare.</param>
            <returns>Returns true if they are not equal and false otherwise.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Matrix4.Equals(System.Object)">
            <summary>
            Compares two Matrix4 objects for equality.
            </summary>
            <param name="obj">The object to compare.</param>
            <returns>Returns true if they are equal and false otherwise.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Matrix4.GetHashCode">
            <summary>
            Gets the hash code for a given Matrix4.
            </summary>
            <returns>The calculated hash code.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Matrix4.Equals(Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Compares two Matrix4 objects for equality.
            </summary>
            <param name="mat">The Matrix4 to compare.</param>
            <returns>Returns true if they are equal and false otherwise.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Matrix4.CopyFrom(Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Returns a Matrix4 that is a copy of a native _Matrix4 structure.
            </summary>
            <param name="mat">The native structure.</param>
            <returns>The managed structure.</returns>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.Identity">
            <summary>
            Gets the identity matrix.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M11">
            <summary>
            Gets or sets Row 1, Column 1.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M12">
            <summary>
            Gets or sets Row 1, Column 2.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M13">
            <summary>
            Gets or sets Row 1, Column 3.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M14">
            <summary>
            Gets or sets Row 1, Column 4.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M21">
            <summary>
            Gets or sets Row 2, Column 1.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M22">
            <summary>
            Gets or sets Row 2, Column 2.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M23">
            <summary>
            Gets or sets Row 2, Column 3.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M24">
            <summary>
            Gets or sets Row 2, Column 4.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M31">
            <summary>
            Gets or sets Row 3, Column 1.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M32">
            <summary>
            Gets or sets Row 3, Column 2.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M33">
            <summary>
            Gets or sets Row 3, Column 3.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M34">
            <summary>
            Gets or sets Row 3, Column 4.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M41">
            <summary>
            Gets or sets Row 4, Column 1.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M42">
            <summary>
            Gets or sets Row 4, Column 2.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M43">
            <summary>
            Gets or sets Row 4, Column 3.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Matrix4.M44">
            <summary>
            Gets or sets Row 4, Column 4.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeArray`1">
            <summary>
            The NativeArray provides the ability to access native array data.
            </summary>
            <typeparam name="T">The data type stores in the native array.</typeparam>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.FusionColorImageFrame">
            <summary>
            A frame used specifically for 32bit RGBA-based images.
            It provides access to the dimensions, format and pixel data for a frame.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.FusionImageFrame">
            <summary>
            The base class for the container of per-frame image data buffers.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionImageFrame.nativeFrame">
            <summary>
            The native image frame structure.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionImageFrame.cameraParams">
            <summary>
            The camera parameters for this frame.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionImageFrame.frameBuffer">
            <summary>
            The native texture frame buffer.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionImageFrame.disposed">
            <summary>
            Track whether Dispose has been called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionImageFrame.#ctor(Microsoft.Kinect.Fusion.FusionImageType,System.Int32,System.Int32,Microsoft.Kinect.Fusion.CameraParameters)">
            <summary>
            Initializes a new instance of the FusionImageFrame class.
            </summary>
            <param name="imageType">The type of image frame to create.</param>
            <param name="width">The width of the image to create.</param>
            <param name="height">The height of the image to create.</param>
            <param name="cameraParameters">The Camera Parameters of the image.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionImageFrame.Finalize">
            <summary>
            Finalizes an instance of the FusionImageFrame class.
            This destructor will run only if the Dispose method does not get called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionImageFrame.ToHandleRef(Microsoft.Kinect.Fusion.FusionImageFrame)">
            <summary>
            Convert a FusionImageFrame to HandleRef structure.
            </summary>
            <param name="imageFrame">The FusionImageFrame to be converted.</param>
            <returns>
            Returns null if the input <para>imageFrame</para> is null or a HandleRef structure.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionImageFrame.Dispose">
            <summary>
            Disposes the FusionImageFrame.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionImageFrame.Dispose(System.Boolean)">
            <summary>
            Frees all memory associated with the FusionImageFrame.
            </summary>
            <param name="disposing">Whether the function was called from Dispose.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionImageFrame.EnsureMarshalled">
            <summary>
            Marshal the image from native if not already marshaled.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.FusionImageFrame.ImageType">
            <summary>
            Gets this frame's type.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.FusionImageFrame.Width">
            <summary>
            Gets this frame's width in pixels.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.FusionImageFrame.Height">
            <summary>
            Gets this frame's height in pixels.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.FusionImageFrame.BytesPerPixel">
            <summary>
            Gets or sets the bytes per pixel of this ImageFrame.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.FusionImageFrame.PixelDataLength">
            <summary>
            Gets the total number of pixels in the buffer of this ImageFrame.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.FusionImageFrame.CameraParameters">
            <summary>
            Gets the camera parameters used by the frame.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.FusionImageFrame.Handle">
            <summary>
            Gets or Sets the native image frame handle.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.FusionImageFrame.Pitch">
            <summary>
            Gets the texture data pitch inside current frame.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.FusionImageFrame.RawBits">
            <summary>
            Gets the texture data inside current frame.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionColorImageFrame.#ctor(System.Int32,System.Int32,Microsoft.Kinect.Fusion.CameraParameters)">
            <summary>
            Initializes a new instance of the FusionColorImageFrame class.
            </summary>
            <param name="width">Image width.</param>
            <param name="height">Image height.</param>
            <param name="cameraParameters">The camera parameters.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionColorImageFrame.#ctor(System.Int32,System.Int32)">
            <summary>
            Initializes a new instance of the FusionColorImageFrame class with default camera parameters.
            </summary>
            <param name="width">Image width.</param>
            <param name="height">Image height.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionColorImageFrame.CopyPixelDataFrom(System.Int32[])">
            <summary>
            This method copies pixel data from a pre-allocated array to this image.
            </summary>
            <param name="sourcePixelData">
            The source int array of pixel data. It must be exactly PixelDataLength pixels in length.
            </param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionColorImageFrame.CopyPixelDataTo(System.Int32[])">
            <summary>
            This method copies pixel data from this frame to a pre-allocated array.
            </summary>
            <param name="destinationPixelData">
            The pixel array to receive the data. The size in pixels must be equal to the frames PixelDataLength.
            </param>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.FusionColorImageFrame.BytesPerPixel">
            <summary>
            Gets the number of bytes per pixel.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.HRESULT">
            <summary>
            The HRESULT defination which is used in Kinect Fusion.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.HRESULT.S_OK">
            <summary>
            The operation succeeded.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.HRESULT.E_NUI_GPU_FAIL">
            <summary>
            The GPU is not capable of running Kinect Fusion or there was an error on initialization.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.HRESULT.E_NUI_BADINDEX">
            <summary>
            The specified index is out of range.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.HRESULT.E_NUI_FUSION_TRACKING_ERROR">
            <summary>
            The iterative tracking algorithm encountered a problem aligning the input point clouds and could not calculate
            a valid transformation.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.HRESULT.E_NUI_FEATURE_NOT_INITIALIZED">
            <summary>
            Kinect SDK Runtime cannot be accessed.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.HRESULT.E_NUI_DEVICE_NOT_CONNECTED">
            <summary>
            Kinect camera not present.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.HRESULT.E_OUTOFMEMORY">
            <summary>
            Out of memory when allocating.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.HRESULT.E_GPU_OUTOFMEMORY">
            <summary>
            Out of memory when allocating on GPU.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.HRESULT.E_INVALIDARG">
            <summary>
            One or more input arguments were null, invalid or outside valid range.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.HRESULT.E_POINTER">
            <summary>
            One or more output arguments were null.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.HRESULT.E_FAIL">
            <summary>
            Generic Error.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.ExceptionHelper">
            <summary>
            Translates HRESULTs from native NUI API's and COM interfaces to managed exceptions.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ExceptionHelper.ThrowIfFailed(Microsoft.Kinect.Fusion.HRESULT)">
            <summary>
            Throws a managed exception for the given native HRESULT from the Fusion API.
            </summary>
            <param name="hr">Native HRESULT returned from API.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ExceptionHelper.CastAndThrowIfOutOfUshortRange(System.Int32)">
            <summary>
            Cast to ushort and throw a managed exception if the int parameter is out of range.
            </summary>
            <param name="intValue">Integer value to cast.</param>
            <returns>On success, returns the int parameter value cast to a ushort.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ExceptionHelper.CastAndThrowIfOutOfUintRange(System.Int32)">
            <summary>
            Cast to uint and throw a managed exception if the int parameter is out of range.
            </summary>
            <param name="intValue">Integer value to cast.</param>
            <returns>On success, returns the int parameter value cast to a uint.</returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.FusionFloatImageFrame">
            <summary>
            A frame used specifically for float-based images.
            It provides access to the dimensions, format and pixel data for a depth frame.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionFloatImageFrame.#ctor(System.Int32,System.Int32,Microsoft.Kinect.Fusion.CameraParameters)">
            <summary>
            Initializes a new instance of the FusionFloatImageFrame class.
            </summary>
            <param name="width">Image width.</param>
            <param name="height">Image height.</param>
            <param name="cameraParameters">The camera parameters.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionFloatImageFrame.#ctor(System.Int32,System.Int32)">
            <summary>
            Initializes a new instance of the FusionFloatImageFrame class with default camera parameters.
            </summary>
            <param name="width">Image width.</param>
            <param name="height">Image height.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionFloatImageFrame.CopyPixelDataFrom(System.Single[])">
            <summary>
             This method copies pixel data from a pre-allocated array to this image.
            </summary>
            <param name="sourcePixelData">
            The source float array of pixel data. It must be exactly PixelDataLength pixels in length.
            </param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionFloatImageFrame.CopyPixelDataTo(System.Single[])">
            <summary>
            This method copies pixel data from this frame to a pre-allocated array.
            </summary>
            <param name="destinationPixelData">
            The destination float array to receive the data. It must be exactly PixelDataLength pixels in length.
            </param>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.FusionFloatImageFrame.BytesPerPixel">
            <summary>
            Gets the bytes per pixel of this image frame.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.FusionDepthProcessor">
            <summary>
            The FusionDepthProcessor class encapsulates all operations on depth processing.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionDepthProcessor.DefaultMinimumDepth">
            <summary>
            The default minimum depth value.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionDepthProcessor.DefaultMaximumDepth">
            <summary>
            The default maximum depth value.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionDepthProcessor.DefaultAlignIterationCount">
            <summary>
            The default align iteration count.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionDepthProcessor.DefaultIntegrationWeight">
            <summary>
            The default integration weight.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionDepthProcessor.DefaultColorIntegrationOfAllAngles">
            <summary>
            The default color integration: no angle restriction, integrate +/-180 degrees (fastest processing).
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionDepthProcessor.GetDeviceInfo(Microsoft.Kinect.Fusion.ReconstructionProcessor,System.Int32,System.String@,System.String@,System.Int32@)">
            <summary>
            Enumerate the devices capable of running KinectFusion.
            This enables a specific device to be chosen when calling NuiFusionCreateReconstruction if desired.
            </summary>
            <param name="type">The type of processor to enumerate.</param>
            <param name="index">
            The zero-based index of the device for which the description is returned.
            or -1 for the default device for the given processor type.
            </param>
            <param name="description">On success, the variable is assigned the description string for the device.</param>
            <param name="instancePath">On success, the variable is assigned the instance path for the device.</param>
            <param name="memoryKB">On success, the variable is assigned the total amount of memory on the device, in kilobytes.</param>
            <exception cref="T:System.IndexOutOfRangeException">
            Thrown when the <paramref name="index"/> parameter is out of range for the specified processor type.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="type"/> parameter is out of range.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionDepthProcessor.DepthToDepthFloatFrame(System.UInt16[],System.Int32,System.Int32,Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Single,System.Single,System.Boolean)">
            <summary>
            Converts Kinect depth frames in unsigned short format to depth frames in float format
            representing distance from the camera in meters (parallel to the optical center axis).
            Note: <paramref name="depthImageData"/> and <paramref name="depthFloatFrame"/> must
            be the same pixel resolution and equal to <paramref name="depthImageDataWidth"/> by
            <paramref name="depthImageDataHeight"/>.
            The min and max depth clip values enable clipping of the input data, for example, to help
            isolate particular objects or surfaces to be reconstructed. Note that the thresholds return 
            different values when a depth pixel is outside the threshold - pixels inside minDepthClip will
            will be returned as 0 and ignored in processing, whereas pixels beyond maxDepthClip will be set
            to 1000 to signify a valid depth ray with depth beyond the set threshold. Setting this far-
            distance flag is important for reconstruction integration in situations where the camera is
            static or does not move significantly, as it enables any voxels closer to the camera
            along this ray to be culled instead of persisting (as would happen if the pixels were simply 
            set to 0 and ignored in processing). Note that when reconstructing large real-world size volumes,
            be sure to set large maxDepthClip distances, as when the camera moves around, any voxels in view
            which go beyond this threshold distance from the camera will be removed.
            </summary>
            <param name="depthImageData">
            An array which stores the extended-depth texture of a depth image from the Kinect camera.
            </param>
            <param name="depthImageDataWidth">Width of the depth image data.</param>
            <param name="depthImageDataHeight">Height of the depth image data.</param>
            <param name="depthFloatFrame">
            A pre-allocated depth float type image frame, to be filled with the floating point depth values.
            </param>
            <param name="minDepthClip">
            Minimum depth distance threshold in meters. Depth pixels below this value will be
            returned as invalid (0). Min depth must be positive or 0.
            </param>
            <param name="maxDepthClip">
            Maximum depth distance threshold in meters. Depth pixels above this value will be
            returned as invalid (1000). Max depth must be greater than 0.
            </param>
            <param name="mirrorDepth">
            A boolean parameter specifying whether to horizontally mirror the input depth image.
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthImageData"/> or the <paramref name="depthFloatFrame"/>
            parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="depthImageDataWidth"/> parameter and depthFloatFrame's
            <c>width</c> is not equal, or the <paramref name="depthImageDataHeight"/> parameter and
            depthFloatFrame's <c>height</c> member is not equal.
            Thrown when the <paramref name="minDepthClip"/> parameter or the
            <paramref name="maxDepthClip"/> is less than zero.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if a CPU memory allocation failed.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionDepthProcessor.DepthFloatFrameToPointCloud(Microsoft.Kinect.Fusion.FusionFloatImageFrame,Microsoft.Kinect.Fusion.FusionPointCloudImageFrame)">
            <summary>
            Construct an oriented point cloud in the local camera frame of reference from a depth float
            image frame. Here we calculate the 3D position of each depth float pixel with the optical
            center of the camera as the origin. We use a right-hand coordinate system, and (in common 
            with bitmap images with top left origin) +X is to the right, +Y down, and +Z is now forward
            from the Kinect camera into the scene, as though looking into the scene from behind the 
            Kinect camera. Both images must be the same size and have the same camera parameters.
            </summary>
            <param name="depthFloatFrame">The depth float frame to be converted.</param>
            <param name="pointCloudFrame">
            A pre-allocated point cloud frame, to be filled with 3D points and normals.
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthFloatFrame"/> or the <paramref name="pointCloudFrame"/>
            parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="depthFloatFrame"/> or <paramref name="pointCloudFrame"/>
            parameters are different image sizes.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if a CPU memory allocation failed.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionDepthProcessor.ShadePointCloud(Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,Microsoft.Kinect.Fusion.Matrix4,Microsoft.Kinect.Fusion.Matrix4,Microsoft.Kinect.Fusion.FusionColorImageFrame,Microsoft.Kinect.Fusion.FusionColorImageFrame)">
            <summary>
            Create a visible color shaded image of a point cloud and its normals. All image
            frames must have the same width and height.
            </summary>
            <param name="pointCloudFrame">The point cloud frame to be shaded.</param>
            <param name="worldToCameraTransform">
            The world to camera transform (camera pose) where the raycast was performed from.
            Pass identity if the point cloud did not originate from a raycast and is in the
            camera local coordinate system.
            </param>
            <param name="worldToBGRTransform">
            A transform mapping the XYZ co-ordinates of world space to a BGR color space,
            used to color surfaces according to their world positions. RGB values are clamped
            to the range [0, 1].
            </param>
            <param name="shadedSurfaceFrame">
            Optionally, a pre-allocated color image frame, to be filled with the shaded
            surface image. Pass null to skip this image.
            </param>
            <param name="shadedSurfaceNormalsFrame">
            Optionally, a pre-allocated color image frame, to be filled with the color shaded
            normals image with color indicating orientation. Pass null to skip this image.
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="pointCloudFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="pointCloudFrame"/> or <paramref name="shadedSurfaceFrame"/>
            or <paramref name="shadedSurfaceNormalsFrame"/> parameters are different image sizes.
            Thrown when the <paramref name="pointCloudFrame"/> or <paramref name="shadedSurfaceFrame"/>
            or <paramref name="shadedSurfaceNormalsFrame"/> parameters have different camera parameters.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if a CPU memory allocation failed.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionDepthProcessor.ShadePointCloud(Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,Microsoft.Kinect.Fusion.Matrix4,Microsoft.Kinect.Fusion.FusionColorImageFrame,Microsoft.Kinect.Fusion.FusionColorImageFrame)">
            <summary>
            Create a visible color shaded image of a point cloud and its normals with simple
            grayscale L.N surface shading. All image frames must have the same width and height.
            </summary>
            <param name="pointCloudFrame">The point cloud frame to be shaded.</param>
            <param name="worldToCameraTransform">
            The world to camera transform (camera pose) where the raycast was performed from.
            Pass identity if the point cloud did not originate from a raycast and is in the
            camera local coordinate system.
            </param>
            <param name="shadedSurfaceFrame">
            Optionally, a pre-allocated color image frame, to be filled with the grayscale L.N 
            shaded surface image. Pass null to skip this image.
            </param>
            <param name="shadedSurfaceNormalsFrame">
            Optionally, a pre-allocated color image frame, to be filled with the color shaded
            normals image with color indicating orientation. Pass null to skip this image.
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="pointCloudFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="pointCloudFrame"/> or <paramref name="shadedSurfaceFrame"/>
            or <paramref name="shadedSurfaceNormalsFrame"/> parameters are different image sizes.
            Thrown when the <paramref name="pointCloudFrame"/> or <paramref name="shadedSurfaceFrame"/>
            or <paramref name="shadedSurfaceNormalsFrame"/> parameters have different camera parameters.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if a CPU memory allocation failed.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionDepthProcessor.AlignPointClouds(Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,System.Int32,Microsoft.Kinect.Fusion.FusionColorImageFrame,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            The AlignPointClouds function uses an iterative algorithm to align two sets of oriented
            point clouds and calculate the camera's relative pose. This is a generic function which
            can be used independently of a Reconstruction Volume with sets of overlapping point clouds.
            All images must be the same size and have the same camera parameters.
            To find the frame-to-frame relative transformation between two sets of point clouds in
            the camera local frame of reference (created by DepthFloatFrameToPointCloud),
            set the <paramref name="observedToReferenceTransform"/> to the identity.
            To calculate the frame-to-model pose transformation between point clouds calculated from 
            new depth frames with DepthFloatFrameToPointCloud and point clouds calculated from an 
            existing Reconstruction volume with CalculatePointCloud (e.g. from the previous frame),
            pass the CalculatePointCloud image as the reference frame, and the current depth frame 
            point cloud from DepthFloatFrameToPointCloud as the observed frame. Set the 
            <paramref name="observedToReferenceTransform"/> to the previous frames calculated camera
            pose that was used in the CalculatePointCloud call.
            Note that here the current frame point cloud will be in the camera local frame of
            reference, whereas the raycast points and normals will be in the global/world coordinate
            system. By passing the <paramref name="observedToReferenceTransform"/> you make the 
            algorithm aware of the transformation between the two coordinate systems.
            The <paramref name="observedToReferenceTransform"/> pose supplied can also take into
            account information you may have from other sensors or sensing mechanisms to aid the
            tracking. To do this multiply the relative frame to frame delta transformation from
            the other sensing system with the previous frame's pose before passing to this function.
            Note that any delta transform used should be in the same coordinate system as that
            returned by the DepthFloatFrameToPointCloud calculation.
            </summary>
            <param name="referencePointCloudFrame">
            The point cloud frame of the reference camera, or the previous Kinect point cloud frame.
            </param>
            <param name="observedPointCloudFrame">
            The point cloud frame of the observed camera, or the current Kinect frame.
            </param>
            <param name="maxAlignIterationCount">
            The maximum number of iterations of the algorithm to run. The minimum value is 1.
            Using only a small number of iterations will have a faster runtime, however, the
            algorithm may not converge to the correct transformation.
            </param>
            <param name="deltaFromReferenceFrame">
            Optionally, a pre-allocated color image frame, to be filled with color-coded data
            from the camera tracking. This may be used as input to additional vision algorithms such as
            object segmentation. Values vary depending on whether the pixel was a valid pixel used in
            tracking (inlier) or failed in different tests (outlier). 0xff000000 indicates an invalid 
            input vertex (e.g. from 0 input depth), or one where no correspondences occur between point
            cloud images. Outlier vertices rejected due to too large a distance between vertices are 
            coded as 0xff008000. Outlier vertices rejected due to to large a difference in normal angle
            between point clouds are coded as 0xff800000. Inliers are color shaded depending on the 
            residual energy at that point, with more saturated colors indicating more discrepancy
            between vertices and less saturated colors (i.e. more white) representing less discrepancy,
            or less information at that pixel. Pass null if this image is not required.
            </param>
            <param name="observedToReferenceTransform">
            A pre-allocated transformation matrix. At entry to the function this should be filled
            with the best guess for the observed to reference transform (usually the last frame's
            calculated pose). At exit this is filled with he calculated pose or identity if the
            calculation failed.
            </param>
            <returns>
            Returns true if successful; returns false if the algorithm encountered a problem aligning
            the input point clouds and could not calculate a valid transformation, and
            the <paramref name="observedToReferenceTransform"/> parameter is set to identity.
            </returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="referencePointCloudFrame"/> or the
            <paramref name="observedPointCloudFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="referencePointCloudFrame"/> or <paramref name="observedPointCloudFrame"/>
            or <paramref name="deltaFromReferenceFrame"/> parameters are different image sizes.
            Thrown when the <paramref name="referencePointCloudFrame"/> or <paramref name="observedPointCloudFrame"/>
            or <paramref name="deltaFromReferenceFrame"/> parameters have different camera parameters.
            Thrown when the <paramref name="maxAlignIterationCount"/> parameter is less than 1.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if a CPU memory allocation failed.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.FusionImageType">
            <summary>
            The Kinect Fusion image types.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionImageType.Invalid">
            <summary>
            Invalid image type.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionImageType.Color">
            <summary>
            RGB32 color data.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionImageType.Float">
            <summary>
            Depth in meters or AlignDepthFloatToReconstruction deltas, as 32bit float data.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.FusionImageType.PointCloud">
            <summary>
            6 floats per pixel (3D Point x,y,z, Normal x,y,z).
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeFrameHandle">
            <summary>
            The native frame pointer wrapper.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.NativeFrameHandle.#ctor">
            <summary>
            Prevents a default instance of the NativeFrameHandle class from being created.
            It is only used for marshalling.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.NativeFrameHandle.ToIntPtr(Microsoft.Kinect.Fusion.NativeFrameHandle)">
            <summary>
            Convert a NativeFrameHandle to IntPtr.
            </summary>
            <param name="frameHandle">The NativeFrameHandle to be converted.</param>
            <returns>
            Returns IntPtr.Zero if the input NativeFrameHandle is null or the underlying IntPtr value.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.NativeFrameHandle.ReleaseHandle">
            <summary>
            Releases the specified frame of image data.
            </summary>
            <returns>
            true if the handle is released successfully; otherwise,
            in the event of a catastrophic failure, false. In this case,
            it generates a ReleaseHandleFailed Managed Debugging Assistant.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.FusionPointCloudImageFrame">
            <summary>
            A frame used specifically for float-based point cloud images.
            It provides access to the dimensions, format and pixel data for a depth frame.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionPointCloudImageFrame.#ctor(System.Int32,System.Int32,Microsoft.Kinect.Fusion.CameraParameters)">
            <summary>
            Initializes a new instance of the FusionPointCloudImageFrame class.
            </summary>
            <param name="width">Image width.</param>
            <param name="height">Image height.</param>
            <param name="cameraParameters">The camera parameters.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionPointCloudImageFrame.#ctor(System.Int32,System.Int32)">
            <summary>
            Initializes a new instance of the FusionPointCloudImageFrame class with default camera parameters.
            </summary>
            <param name="width">Image width.</param>
            <param name="height">Image height.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionPointCloudImageFrame.CopyPixelDataFrom(System.Single[])">
            <summary>
             This method copies pixel data from a pre-allocated array to this image.
            </summary>
            <param name="sourcePixelData">
            The source float array of pixel data. It must be exactly PixelDataLength pixels in length,
            with the number of bytes per Pixel equal to BytesPerPixel.
            </param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.FusionPointCloudImageFrame.CopyPixelDataTo(System.Single[])">
            <summary>
            This method copies pixel data from this frame to a pre-allocated array.
            </summary>
            <param name="destinationPixelData">
            The destination float array to receive the data. It must be exactly PixelDataLength pixels
            in length, with the number of bytes per Pixel equal to BytesPerPixel.
            </param>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.FusionPointCloudImageFrame.BytesPerPixel">
            <summary>
            Gets the bytes per pixel of this image frame.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.Mesh">
            <summary>
            The Mesh object is created when meshing a Reconstruction volume. This provides access to the vertices,
            normals and triangle indexes of the mesh.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.Mesh.vertices">
            <summary>
            The vertices read only collection.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.Mesh.normals">
            <summary>
            The normals read only collection.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.Mesh.triangleIndexes">
            <summary>
            The triangle indexes read only collection.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.Mesh.mesh">
            <summary>
            The native INuiFusionMesh interface wrapper.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.Mesh.disposed">
            <summary>
            Track whether Dispose has been called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Mesh.#ctor(Microsoft.Kinect.Fusion.INuiFusionMesh)">
            <summary>
            Initializes a new instance of the Mesh class.
            </summary>
            <param name="mesh">The mesh interface to be encapsulated.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Mesh.Finalize">
            <summary>
            Finalizes an instance of the Mesh class.
            This destructor will run only if the Dispose method does not get called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Mesh.GetVertices">
            <summary>
            Gets the collection of vertices. Each vertex has a corresponding normal with the same index.
            </summary>
            <returns>Returns a reference to the read only collection of the vertices.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Mesh.GetNormals">
            <summary>
            Gets the collection of normals. Each normal has a corresponding vertex with the same index.
            </summary>
            <returns>Returns a reference to the read only collection of the normals.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Mesh.GetTriangleIndexes">
            <summary>
            Gets the collection of triangle indexes. There are 3 indexes per triangle.
            </summary>
            <returns>Returns a reference to the read only collection of the triangle indexes.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Mesh.Dispose">
            <summary>
            Disposes the Mesh.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Mesh.Dispose(System.Boolean)">
            <summary>
            Dispose of the native Mesh object.
            </summary>
            <param name="disposing">Whether the function was called from Dispose.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionMesh.VertexCount">
            <summary>
            Gets the number of vertices in the mesh (i.e. the number of elements in the array
            returned when calling GetVertices).
            </summary>
            <returns>
            Returns the number of vertices in the mesh.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionMesh.GetVertices(System.IntPtr@)">
            <summary>
            Gets the vertices. Each vertex has a corresponding normal with the same index.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionMesh.NormalCount">
            <summary>
            Gets the number of normals in the mesh (i.e. the number of elements in the array
            returned when calling GetNormals).
            </summary>
            <returns>
            Returns the number of normals in the mesh.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionMesh.GetNormals(System.IntPtr@)">
            <summary>
            Gets the normals. Each normal has a corresponding vertex with the same index.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionMesh.TriangleVertexIndexCount">
            <summary>
            Gets the number of triangle indices in the mesh (i.e. the number of elements in the array
            returned when calling GetTriangleIndices). Each triangle is formed by three consecutive
            indices, used to index the vertex and normal buffers.
            </summary>
            <returns>
            Returns the length of the buffer.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionMesh.GetTriangleIndices(System.IntPtr@)">
            <summary>
            Gets the triangle indices.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorMesh.VertexCount">
            <summary>
            Gets the number of vertices in the mesh (i.e. the number of elements in the array
            returned when calling GetVertices).
            </summary>
            <returns>
            Returns the number of vertices in the mesh.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorMesh.GetVertices(System.IntPtr@)">
            <summary>
            Gets the vertices. Each vertex has a corresponding normal with the same index.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorMesh.NormalCount">
            <summary>
            Gets the number of normals in the mesh (i.e. the number of elements in the array
            returned when calling GetNormals).
            </summary>
            <returns>
            Returns the number of normals in the mesh.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorMesh.GetNormals(System.IntPtr@)">
            <summary>
            Gets the normals. Each normal has a corresponding vertex with the same index.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorMesh.TriangleVertexIndexCount">
            <summary>
            Gets the number of triangle indices in the mesh (i.e. the number of elements in the array
            returned when calling GetTriangleIndices). Each triangle is formed by three consecutive
            indices, used to index the vertex and normal buffers.
            </summary>
            <returns>
            Returns the length of the buffer.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorMesh.GetTriangleIndices(System.IntPtr@)">
            <summary>
            Gets the triangle indices.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorMesh.ColorCount">
            <summary>
            Gets the number of colors in the mesh (i.e. the number of elements in the array
            returned when calling GetColors). When capturing color, each vertex will an associated 
            color.
            </summary>
            <returns>
            Returns the length of the buffer.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorMesh.GetColors(System.IntPtr@)">
            <summary>
            Gets the colors. Each vertex has a corresponding color with the same index.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.ResetReconstruction(Microsoft.Kinect.Fusion.Matrix4@,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Clear the volume, optionally setting a new initial camera pose and worldToVolumeTransform.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.AlignDepthFloatToReconstruction(System.Runtime.InteropServices.HandleRef,System.UInt16,System.Runtime.InteropServices.HandleRef,System.Single@,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Aligns a depth float image to the Reconstruction volume to calculate the new camera pose.
            This camera tracking method requires a Reconstruction volume, and updates the internal 
            camera pose if successful.
            The maximum image resolution supported in this function is 640x480.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.GetCurrentWorldToCameraTransform(Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Get current internal camera pose.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.GetCurrentWorldToVolumeTransform(Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Get current internal world to volume transform.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.IntegrateFrame(System.Runtime.InteropServices.HandleRef,System.UInt16,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Integrates depth float data into the reconstruction volume using the current internal 
            camera pose, or the optional pWorldToCameraTransform camera pose.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.ProcessFrame(System.Runtime.InteropServices.HandleRef,System.UInt16,System.UInt16,System.Single@,Microsoft.Kinect.Fusion.Matrix4@)">
             <summary>
             A high-level function to process a depth frame through the Kinect Fusion pipeline.
             Specifically, this performs on-GPU processing equivalent to the following functions 
             for each frame:
            
             1) AlignDepthFloatToReconstruction
             2) IntegrateFrame
            
             Users may also optionally call the low-level functions individually, instead of calling this
             function, for more control. However, this function call will be faster due to the integrated 
             nature of the calls. After this call completes, if a visible output image of the reconstruction
             is required, the user can call CalculatePointCloud and then FusionDepthProcessor.ShadePointCloud.
             The maximum image resolution supported in this function is 640x480.
            
             If there is a tracking error in the AlignDepthFloatToReconstruction stage, no depth data 
             integration will be performed, and the camera pose will remain unchanged.
             </summary>
             <returns>
             Returns S_OK if successful; otherwise, returns the error code.
             </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.CalculatePointCloud(System.Runtime.InteropServices.HandleRef,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Calculate a point cloud by raycasting into the reconstruction volume, returning the point
            cloud containing 3D points and normals of the zero-crossing dense surface at every visible 
            pixel in the image from the given camera pose.
            This point cloud can be used as a reference frame in the next call to 
            FusionDepthProcessor.AlignPointClouds, or passed to FusionDepthProcessor.ShadePointCloud 
            to produce a visible image output. 
            pointCloudFrame can be an arbitrary image size, for example, enabling you to calculate 
            point clouds at the size of your window and then create a visible image by calling 
            ShadePointCloud and render this image. However, large images will be expensive to calculate.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.CalculateMesh(System.UInt32,Microsoft.Kinect.Fusion.INuiFusionMesh@)">
            <summary>
            Export a mesh of the zero-crossing dense surfaces in the reconstruction volume.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.ExportVolumeBlock(System.UInt32,System.UInt32,System.UInt32,System.UInt32,System.UInt32,System.UInt32,System.UInt32,System.UInt32,System.Int16[])">
            <summary>
            Export a part or all of the reconstruction volume as a short array. 
            The surface boundary occurs where the tri-linearly interpolated voxel values have a zero crossing.
            Note, this means that a 0 in the volume does not necessarily imply a surface.  A surface only 
            occurs when an interpolation crosses from positive to negative or vice versa.   
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.ImportVolumeBlock(System.UInt32,System.Int16[])">
            <summary>
            Import a reconstruction volume as a short array. 
            This array must equal the size of the initialized reconstruction volume.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.DepthToDepthFloatFrame(System.UInt16[],System.UInt32,System.Runtime.InteropServices.HandleRef,System.Single,System.Single,System.Boolean)">
            <summary>
            Converts Kinect depth frames in unsigned short format to depth frames in float format 
            representing distance from the camera in meters (parallel to the optical center axis).
            Note: <paramref name="depthImageData"/> and <paramref name="depthFloatFrame"/> must
            be the same pixel resolution. This version of the function runs on the GPU.
            The min and max depth clip values enable clipping of the input data, for example, to help
            isolate particular objects or surfaces to be reconstructed. Note that the thresholds return 
            different values when a depth pixel is outside the threshold - pixels inside minDepthClip will
            will be returned as 0 and ignored in processing, whereas pixels beyond maxDepthClip will be set
            to 1000 to signify a valid depth ray with depth beyond the set threshold. Setting this far-
            distance flag is important for reconstruction integration in situations where the camera is
            static or does not move significantly, as it enables any voxels closer to the camera
            along this ray to be culled instead of persisting (as would happen if the pixels were simply 
            set to 0 and ignored in processing). Note that when reconstructing large real-world size volumes,
            be sure to set large maxDepthClip distances, as when the camera moves around, any voxels in view
            which go beyond this threshold distance from the camera will be removed.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.SmoothDepthFloatFrame(System.Runtime.InteropServices.HandleRef,System.Runtime.InteropServices.HandleRef,System.UInt32,System.Single)">
            <summary>
            Spatially smooth a depth float image frame using edge-preserving filtering on GPU. 
            </summary>
            <param name="depthFloatFrame">A pointer to a pre-allocated depth float frame.</param>
            <param name="smoothDepthFloatFrame">A pointer to a pre-allocated depth float frame,
            to be filled with smoothed depth.</param>
            <param name="kernelWidth">Smoothing Kernel Width. Valid values are  1,2,3 
            (for 3x3,5x5,7x7 smoothing kernel block size respectively).</param>
            <param name="distanceThreshold">A distance difference range that smoothing occurs in.
            Pixels with neighboring pixels outside this distance range will not be smoothed 
            (larger values indicate discontinuity/edge). Must be greater than 0.</param>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.AlignPointClouds(System.Runtime.InteropServices.HandleRef,System.Runtime.InteropServices.HandleRef,System.UInt16,System.Runtime.InteropServices.HandleRef,System.Single@,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            The AlignPointClouds function uses an on GPU iterative algorithm to align two sets of 
            overlapping oriented point clouds and calculate the camera's relative pose.
            All images must be the same size and have the same camera parameters. 
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.SetAlignDepthFloatToReconstructionReferenceFrame(System.Runtime.InteropServices.HandleRef)">
            <summary>
            Set a reference depth frame to be used internally to help with tracking when calling 
            AlignDepthFloatToReconstruction to calculate a new camera pose. This function should
            only be called when not using the default tracking behavior of Kinect Fusion.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionReconstruction.CalculatePointCloudAndDepth(System.Runtime.InteropServices.HandleRef,System.Runtime.InteropServices.HandleRef,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Calculate a point cloud and depth image by raycasting into the reconstruction volume. 
            This returns the point cloud containing 3D points and normals of the zero-crossing dense 
            surface at every visible pixel in the image from the given camera pose. This point cloud
            can then be used as a reference frame in the next call to 
            FusionDepthProcessor.AlignPointClouds, or passed to FusionDepthProcessor.ShadePointCloud
            to produce a visible image output.
            pointCloudFrame and depthFloatFrame can be an arbitrary image size, for example, enabling 
            you to calculate point clouds at the size of your window and then create a visible image 
            by calling FusionDepthProcessor.ShadePointCloud and render this image. However, large 
            images will be expensive to calculate.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.ResetReconstruction(Microsoft.Kinect.Fusion.Matrix4@,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Clear the volume, optionally setting a new initial camera pose and worldToVolumeTransform.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.AlignDepthFloatToReconstruction(System.Runtime.InteropServices.HandleRef,System.UInt16,System.Runtime.InteropServices.HandleRef,System.Single@,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Aligns a depth float image to the Reconstruction volume to calculate the new camera pose.
            This camera tracking method requires a Reconstruction volume, and updates the internal 
            camera pose if successful.
            The maximum image resolution supported in this function is 640x480.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.GetCurrentWorldToCameraTransform(Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Get current internal camera pose.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.GetCurrentWorldToVolumeTransform(Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Get current internal world to volume transform.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.IntegrateFrame(System.Runtime.InteropServices.HandleRef,System.Runtime.InteropServices.HandleRef,System.UInt16,System.Single,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Integrates depth float data into the reconstruction volume using the current internal 
            camera pose, or the optional worldToCameraTransform camera pose. Also constrains 
            integration to integrate color over a given angle in degrees relative to the surface normal 
            (recommended use is for thin structure scanning).
            Note: Pass FusionDepthProcessor.DefaultColorIntegrationOfAllAngles to ignore the angle parameter
            and accept color from all angles (default, fastest processing).
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.ProcessFrame(System.Runtime.InteropServices.HandleRef,System.Runtime.InteropServices.HandleRef,System.UInt16,System.UInt16,System.Single,System.Single@,Microsoft.Kinect.Fusion.Matrix4@)">
             <summary>
             A high-level function to process a depth frame through the Kinect Fusion pipeline.
             Optionally also integrates color, further also constrains integration to integrate 
             color over a given angle in degrees relative to the surface normal (recommended use is for
             thin structure scanning).
             Note: Pass FusionDepthProcessor.DefaultColorIntegrationOfAllAngles to ignore the angle parameter
             and accept color from all angles (default, fastest processing).
             Specifically, this performs on-GPU processing equivalent to the following functions 
             for each frame:
            
             1) AlignDepthFloatToReconstruction
             2) IntegrateFrame
            
             Users may also optionally call the low-level functions individually, instead of calling this
             function, for more control. However, this function call will be faster due to the integrated 
             nature of the calls. After this call completes, if a visible output image of the reconstruction
             is required, the user can call CalculatePointCloud and then FusionDepthProcessor.ShadePointCloud.
             The maximum image resolution supported in this function is 640x480.
            
             If there is a tracking error in the AlignDepthFloatToReconstruction stage, no depth data 
             integration will be performed, and the camera pose will remain unchanged.
             </summary>
             <returns>
             Returns S_OK if successful; otherwise, returns the error code.
             </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.CalculatePointCloud(System.Runtime.InteropServices.HandleRef,System.Runtime.InteropServices.HandleRef,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Calculate a point cloud by raycasting into the reconstruction volume, returning the point
            cloud containing 3D points and normals of the zero-crossing dense surface at every visible 
            pixel in the image from the given camera pose.
            This point cloud can be used as a reference frame in the next call to 
            FusionDepthProcessor.AlignPointClouds, or passed to FusionDepthProcessor.ShadePointCloud to
            produce a visible image output. 
            pPointCloudFrame can be an arbitrary image size, for example, enabling you to calculate 
            point clouds at the size of your window and then create a visible image by calling 
            FusionDepthProcessor.ShadePointCloud and render this image. However, large images will be 
            expensive to calculate.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.CalculateMesh(System.UInt32,Microsoft.Kinect.Fusion.INuiFusionColorMesh@)">
            <summary>
            Export a mesh of the zero-crossing dense surfaces in the reconstruction volume,
            with per-vertex color.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.ExportVolumeBlock(System.UInt32,System.UInt32,System.UInt32,System.UInt32,System.UInt32,System.UInt32,System.UInt32,System.UInt32,System.UInt32,System.Int16[],System.Int32[])">
            <summary>
            Export a part or all of the reconstruction volume as a short array and color as int array.
            The surface boundary occurs where the tri-linearly interpolated voxel values have a zero crossing.
            Note, this means that a 0 in the volume does not necessarily imply a surface.  A surface only 
            occurs when an interpolation crosses from positive to negative or vice versa.   
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.ImportVolumeBlock(System.UInt32,System.UInt32,System.Int16[],System.Int32[])">
            <summary>
            Import a reconstruction volume as a short array and color as int array.
            This array must equal the size of the initialized reconstruction volume.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.DepthToDepthFloatFrame(System.UInt16[],System.UInt32,System.Runtime.InteropServices.HandleRef,System.Single,System.Single,System.Boolean)">
            <summary>
            Converts Kinect depth frames in unsigned short format to depth frames in float format 
            representing distance from the camera in meters (parallel to the optical center axis).
            Note: <paramref name="depthImageData"/> and <paramref name="depthFloatFrame"/> must
            be the same pixel resolution. This version of the function runs on the GPU.
            The min and max depth clip values enable clipping of the input data, for example, to help
            isolate particular objects or surfaces to be reconstructed. Note that the thresholds return 
            different values when a depth pixel is outside the threshold - pixels inside minDepthClip will
            will be returned as 0 and ignored in processing, whereas pixels beyond maxDepthClip will be set
            to 1000 to signify a valid depth ray with depth beyond the set threshold. Setting this far-
            distance flag is important for reconstruction integration in situations where the camera is
            static or does not move significantly, as it enables any voxels closer to the camera
            along this ray to be culled instead of persisting (as would happen if the pixels were simply 
            set to 0 and ignored in processing). Note that when reconstructing large real-world size volumes,
            be sure to set large maxDepthClip distances, as when the camera moves around, any voxels in view
            which go beyond this threshold distance from the camera will be removed.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.SmoothDepthFloatFrame(System.Runtime.InteropServices.HandleRef,System.Runtime.InteropServices.HandleRef,System.UInt32,System.Single)">
            <summary>
            Spatially smooth a depth float image frame using edge-preserving filtering on GPU. 
            </summary>
            <param name="depthFloatFrame">A pointer to a pre-allocated depth float frame.</param>
            <param name="smoothDepthFloatFrame">A pointer to a pre-allocated depth float frame,
            to be filled with smoothed depth.</param>
            <param name="kernelWidth">Smoothing Kernel Width. Valid values are  1,2,3 
            (for 3x3,5x5,7x7 smoothing kernel block size respectively).</param>
            <param name="distanceThreshold">A distance difference range that smoothing occurs in.
            Pixels with neighboring pixels outside this distance range will not be smoothed 
            (larger values indicate discontinuity/edge). Must be greater than 0.</param>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.AlignPointClouds(System.Runtime.InteropServices.HandleRef,System.Runtime.InteropServices.HandleRef,System.UInt16,System.Runtime.InteropServices.HandleRef,System.Single@,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            The AlignPointClouds function uses an on GPU iterative algorithm to align two sets of 
            overlapping oriented point clouds and calculate the camera's relative pose.
            All images must be the same size and have the same camera parameters. 
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.SetAlignDepthFloatToReconstructionReferenceFrame(System.Runtime.InteropServices.HandleRef)">
            <summary>
            Set a reference depth frame to be used internally to help with tracking when calling 
            AlignDepthFloatToReconstruction to calculate a new camera pose. This function should
            only be called when not using the default tracking behavior of Kinect Fusion.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionColorReconstruction.CalculatePointCloudAndDepth(System.Runtime.InteropServices.HandleRef,System.Runtime.InteropServices.HandleRef,System.Runtime.InteropServices.HandleRef,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            Calculate a point cloud and depth image by raycasting into the reconstruction volume. 
            This returns the point cloud containing 3D points and normals of the zero-crossing dense 
            surface at every visible pixel in the image from the given camera pose. This point cloud
            can then be used as a reference frame in the next call to 
            FusionDepthProcessor.AlignPointClouds, or passed to FusionDepthProcessor.ShadePointCloud
            to produce a visible image output.
            pointCloudFrame and depthFloatFrame can be an arbitrary image size, for example, enabling 
            you to calculate point clouds at the size of your window and then create a visible image 
            by calling FusionDepthProcessor.ShadePointCloud and render this image. However, large 
            images will be expensive to calculate.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionMatchCandidates.MatchPoseCount">
            <summary>
            Gets the number of camera poses matched to the input in the camera pose finder.
            </summary>
            <returns>
            Returns the number of Matrix4 poses.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionMatchCandidates.GetMatchPoses(System.IntPtr@)">
            <summary>
            Gets the camera poses matched to the input in the camera pose finder. The poses are sorted in
            terms of descending similarity (i.e. the most similar is first).
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionMatchCandidates.MatchSimilarityCount">
            <summary>
            Gets the number of camera pose similarity measurements to the input in the camera pose finder.
            </summary>
            <returns>
            Returns the number of similarity measurements (equal to number of poses).
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionMatchCandidates.GetMatchSimilarities(System.IntPtr@)">
            <summary>
            Gets the matched frame similarity measurements, indicating how similar the match is to the input
            in the camera pose finder. These measurements correspond to the respective poses returned in 
            GetMatchPoses and are sorted in terms of descending similarity (i.e. the most similar is first).
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionMatchCandidates.CalculateMinimumDistance(System.Single@)">
            <summary>
            Calculate the minimum distance between the input and the single most similar pose frame.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionCameraPoseFinder.ResetCameraPoseFinder">
            <summary>
            Clear the camera pose finder database information.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionCameraPoseFinder.ProcessFrame(System.Runtime.InteropServices.HandleRef,System.Runtime.InteropServices.HandleRef,Microsoft.Kinect.Fusion.Matrix4@,System.Single,System.Boolean@,System.Boolean@)">
            <summary>
            Test input camera frames against the camera pose finder database, adding frames to the
            database if dis-similar enough to existing frames. Both input depth and color frames must 
            be identical sizes, with valid camera parameters, and captured at the same time.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionCameraPoseFinder.FindCameraPose(System.Runtime.InteropServices.HandleRef,System.Runtime.InteropServices.HandleRef,Microsoft.Kinect.Fusion.INuiFusionMatchCandidates@)">
            <summary>
            Test input camera frames against the camera pose finder database, returning a set of similar
            camera poses in <paramref name="matchCandidates"/>. These poses and similarity measurements
            are ordered in terms of descending similarity (i.e. the most similar is first).
            Both input depth and color frames must be identical sizes, with valid camera parameters
            and captured at the same time.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionCameraPoseFinder.LoadCameraPoseFinderDatabase(System.String)">
            <summary>
            Load a previously saved camera pose finder database from disk. 
            Note: All existing camera pose finder data is replaced on a successful load of the database.
            If the database is saved to disk alongside the reconstruction volume, when both are
            re-loaded, this potentially enables reconstruction and tracking to be re-started and
            the reconstruction updated by running the camera pose finder.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionCameraPoseFinder.SaveCameraPoseFinderDatabase(System.String)">
            <summary>
            Save the camera pose finder database to disk. 
            If the database is saved to disk alongside the reconstruction volume, when both are
            re-loaded, this potentially enables reconstruction and tracking to be re-started and
            the reconstruction updated by running the camera pose finder.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionCameraPoseFinder.GetCameraPoseFinderParameters(Microsoft.Kinect.Fusion.CameraPoseFinderParameters)">
            <summary>
            Get the parameters used to create the camera pose finder database.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.INuiFusionCameraPoseFinder.GetStoredPoseCount">
            <summary>
            Get the number of poses stored in the camera pose finder database.
            </summary>
            <returns>
            Returns the number of stored poses.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NUI_FUSION_IMAGE_FRAME">
            <summary>
            The native NUI_FUSION_IMAGE_FRAME structure.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NUI_FUSION_IMAGE_FRAME.Width">
            <summary>
            The width of the image frame.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NUI_FUSION_IMAGE_FRAME.Height">
            <summary>
            The height of the image frame.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NUI_FUSION_IMAGE_FRAME.ImageType">
            <summary>
            The image frame type.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NUI_FUSION_IMAGE_FRAME.CameraParameters">
            <summary>
            The pointer to the camera parameters.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NUI_FUSION_IMAGE_FRAME.FrameBuffer">
            <summary>
            The pointer to the image frame buffer.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NUI_FUSION_FRAME_BUFFER">
            <summary>
            The native NUI_FUSION_FRAME_BUFFER structure.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NUI_FUSION_FRAME_BUFFER.Pitch">
            <summary>
            The pitch of the frame buffer.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NUI_FUSION_FRAME_BUFFER.RawBits">
            <summary>
            The pointer to the buffer data.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods">
            <summary>
            Native API declarations from KinectFusion dll.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.CreateImageFrame">
            <summary>
            The delegate for NuiFusionCreateImageFrame function.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.ReleaseImageFrame">
            <summary>
            The delegate for NuiFusionReleaseImageFrame function.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.DepthToDepthFloatFrame">
            <summary>
            The delegate for NuiFusionDepthToDepthFloatFrame function.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.DepthFloatFrameToPointCloud">
            <summary>
            The delegate for NuiFusionDepthFloatFrameToPointCloud function.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.ShadePointCloud">
            <summary>
            A delegate for NuiFusionShadePointCloud function.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.ShadePointCloud2">
            <summary>
            A delegate for NuiFusionShadePointCloud function.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.AlignPointClouds">
            <summary>
            The delegate for NuiFusionAlignPointClouds function.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.CreateReconstruction">
            <summary>
            The delegate for NuiFusionCreateReconstruction function.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.CreateColorReconstruction">
            <summary>
            The delegate for NuiFusionCreateColorReconstruction function.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.GetDeviceInfo">
            <summary>
            The delegate for NuiFusionGetDeviceInfo function.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.CreateCameraPoseFinder">
            <summary>
            The delegate for NuiFusionCreateCameraPoseFinder function.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.CreateCameraPoseFinder2">
            <summary>
            The delegate for NuiFusionCreateCameraPoseFinder function to handle null.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.NativeMethods.fusionModule">
            <summary>
            The native KinectFusion dll handle.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.NativeMethods.LoadLibrary(System.String)">
            <summary>
            Loads the specified dll into the address space of the calling process.
            </summary>
            <returns>
            If the function succeeds, the return value is a handle to the module.
            If the function fails, the return value is NULL. To get extended error information, call GetLastError
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.NativeMethods.GetProcAddress(System.IntPtr,System.String)">
            <summary>
            Retrieves the address of an exported function or variable from the specified
            dynamic-link library (DLL).
            </summary>
            <returns>
            If the function succeeds, the return value is a handle to the module.
            If the function fails, the return value is NULL. To get extended error information, call GetLastError
            </returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.NativeMethods.GetFunctionDelegate``1(System.String)">
            <summary>
            Get the delegate of a native function.
            </summary>
            <typeparam name="FunctionType">The function delegate type.</typeparam>
            <param name="functionName">The native function name to be loaded.</param>
            <returns>The delegate of the function.</returns>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionCreateImageFrame">
            <summary>
            Get the delegate for the native NuiFusionCreateImageFrame function.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionReleaseImageFrame">
            <summary>
            Get the delegate for the native NuiFusionReleaseImageFrame function.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionDepthToDepthFloatFrame">
            <summary>
            Get the delegate for the native NuiFusionDepthToDepthFloatFrame function.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionDepthFloatFrameToPointCloud">
            <summary>
            Get the delegate for the native NuiFusionDepthFloatFrameToPointCloud function.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionShadePointCloud">
            <summary>
            Get the delegate for the native NuiFusionShadePointCloud function.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionShadePointCloud2">
            <summary>
            Get the delegate for the native NuiFusionShadePointCloud2 function.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionAlignPointClouds">
            <summary>
            Get the delegate for the native NuiFusionAlignPointClouds function.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionCreateReconstruction">
            <summary>
            Get the delegate for the native NuiFusionCreateReconstruction function.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionCreateColorReconstruction">
            <summary>
            Get the delegate for the native NuiFusionCreateColorReconstruction function.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionGetDeviceInfo">
            <summary>
            Get the delegate for the native NuiFusionGetDeviceInfo function.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionCreateCameraPoseFinder">
            <summary>
            Get the delegate for the native NuiFusionCreateCameraPoseFinder function.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionCreateCameraPoseFinder2">
            <summary>
            Get the delegate for the native NuiFusionCreateCameraPoseFinder function.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.NativeMethods.LibraryName">
            <summary>
            Get the library name for current execution architecture.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionCreateImageFrameDelegate">
            <summary>
            Create an image frame for use with Kinect Fusion with a specified data type and resolution.
            Note that image width must be a minimum of 32 pixels in both width and height and for camera 
            tracking Align functions and volume Integration, use of the default camera parameters is only 
            supported with 4:3 pixel aspect ratio images such as uniformly scaled versions of the source Kinect 
            image (e.g. 160x120,320x240,640x480 etc.). To crop to smaller non 4:3 ratios and still use the 
            default camera parameters set unwanted pixels to 0 depth, which will be ignored in processing, or 
            alternately, the user can supply their own calibration with an arbitrary sized image. For example,
            a user supplied set of parameters can be used when calling CalculatePointCloud to calculate a 
            large image of the reconstruction at the UI window resolution (perhaps with a virtual viewpoint 
            different to the Kinect camera or a non 4:3 aspect image ratio) by then subsequently calling 
            ShadePointCloud and rendering the resulting images on screen.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionReleaseImageFrameDelegate">
            <summary>
            Releases the specified frame of data.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionDepthToDepthFloatFrameDelegate">
            <summary>
            Converts Kinect depth frames in unsigned short format to depth frames in float format 
            representing distance from the camera in meters (parallel to the optical center axis).
            Note: <paramref name="depthImageData"/> and <paramref name="depthFloatFrame"/> must
            be the same pixel resolution and equal to <paramref name="depthImageDataWidth"/> by
            <paramref name="depthImageDataHeight"/>.
            The min and max depth clip values enable clipping of the input data, for example, to help
            isolate particular objects or surfaces to be reconstructed. Note that the thresholds return 
            different values when a depth pixel is outside the threshold - pixels inside minDepthClip will
            will be returned as 0 and ignored in processing, whereas pixels beyond maxDepthClip will be set
            to 1000 to signify a valid depth ray with depth beyond the set threshold. Setting this far-
            distance flag is important for reconstruction integration in situations where the camera is
            static or does not move significantly, as it enables any voxels closer to the camera
            along this ray to be culled instead of persisting (as would happen if the pixels were simply 
            set to 0 and ignored in processing). Note that when reconstructing large real-world size volumes,
            be sure to set large maxDepthClip distances, as when the camera moves around, any voxels in view
            which go beyond this threshold distance from the camera will be removed.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionDepthFloatFrameToPointCloudDelegate">
            <summary>
            Construct an oriented point cloud in the local camera frame of reference from a depth float
            image frame. Here we calculate the 3D position of each depth float pixel with the optical
            center of the camera as the origin. Both images must be the same size and have the same camera
            parameters. We use a right-hand coordinate system, and (in common with bitmap images with top left
            origin) +X is to the right, +Y down, and +Z is now forward from the Kinect camera into the scene,
            as though looking into the scene from behind the kinect.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionShadePointCloudDelegate">
            <summary>
            Create visible color shaded images of a point cloud and its normals. All image frames must be
            the same size and have the same camera parameters.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionShadePointCloud2Delegate">
            <summary>
            Create visible color shaded images of a point cloud and its normals. All image frames must be
            the same size and have the same camera parameters.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionAlignPointCloudsDelegate">
            <summary>
            The AlignPointClouds function uses an iterative algorithm to align two sets of oriented point 
            clouds and calculate the camera's relative pose. This is a generic function which can be used
            independently of a Reconstruction Volume with sets of overlapping point clouds.
            All images must be the same size and have the same camera parameters.
            To find the frame to frame relative transformation between two sets of point clouds in the 
            camera local frame of reference (created by NuiFusionDepthFloatFrameToPointCloud), set the
            referenceToObservedTransform to NULL or the identity.
            To calculate the pose transformation between new depth frames and an existing Reconstruction
            volume, pass in previous frame's point cloud from CalculatePointCloud as the reference frame,
            and the current frame point cloud (from NuiFusionDepthFloatFrameToPointCloud) as the observed
            frame. Set the referenceToObservedTransform to the previous frames calculated camera pose.
            Note that here the current frame point cloud will be in the camera local frame of reference,
            whereas the synthetic points and normals will be in the global/world volume coordinate system.
            By passing the referenceToObservedTransform you make the algorithm aware of the transformation
            between them.
            The referenceToObservedTransform pose supplied can also take into account information you may
            have from other sensors or sensing mechanisms to aid the tracking. 
            To do this multiply the relative frame to frame delta transformation from the other sensing
            system with the previous frame's pose before passing to this function.
            Note that any delta transform used should be in the same coordinate system as that returned 
            by the NuiFusionDepthFloatFrameToPointCloud calculation.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionCreateReconstructionDelegate">
            <summary>
            Create Kinect Fusion Reconstruction Volume instance.
            Voxel volume axis sizes must be greater than 0 and a multiple of 32.
            Users can select which device the processing is performed on with the reconstructionProcessorType parameter.
            For those with multiple devices the deviceIndex parameter also enables users to explicitly configure on 
            which device the reconstruction volume is created.
            Note that this function creates a default world-volume transform. To set a non-default
            transform call ResetReconstruction with an appropriate Matrix4. This default transform is a
            translation and scale to locate the world origin at the center of the front face of the volume
            and set the relationship between the the world coordinates and volume indices.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionCreateColorReconstructionDelegate">
            <summary>
            Create Kinect Fusion Reconstruction Volume instance enabling use of in-volume color.
            Voxel volume axis sizes must be greater than 0 and a multiple of 32.
            Users can select which device the processing is performed on with the reconstructionProcessorType parameter.
            For those with multiple devices the deviceIndex parameter also enables users to explicitly configure on 
            which device the reconstruction volume is created.
            Note that this function creates a default world-volume transform. To set a non-default
            transform call ResetReconstruction with an appropriate Matrix4. This default transform is a
            translation and scale to locate the world origin at the center of the front face of the volume
            and set the relationship between the the world coordinates and volume indices.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionGetDeviceInfoDelegate">
            <summary>
            Enumerate the devices capable of running KinectFusion.
            This enables a specific device to be chosen when calling NuiFusionCreateReconstruction if desired.
            </summary>
            <param name="type">The type of processor to enumerate.</param>
            <param name="index">The zero-based index of the device for which the description is returned.</param>
            <param name="description">A buffer that receives a description string for the device.</param>
            <param name="descriptionSizeInChar">The size of the buffer referenced by <paramref name="description"/>, in characters.</param>
            <param name="instancePath">A buffer that receives the device instance string.</param>
            <param name="instancePathSizeInChar">The size of the buffer referenced by <paramref name="instancePath"/>, in characters.</param>
            <param name="memoryKB">On success, the variable is assigned the total amount of memory on the device, in kilobytes.</param>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionCreateCameraPoseFinderDelegate">
            <summary>
            Create Kinect Fusion camera pose finder instance.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.NativeMethods.NuiFusionCreateCameraPoseFinder2Delegate">
            <summary>
            Create Kinect Fusion camera pose finder instance 2 to handle null.
            </summary>
            <returns>
            Returns S_OK if successful; otherwise, returns the error code.
            </returns>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.ReconstructionProcessor">
            <summary>
            The Reconstruction Processor type.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.ReconstructionProcessor.Cpu">
            <summary>
            Process all Reconstruction calls on CPU (potentially enables larger volume allocation,
            but not real-time processing)
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.ReconstructionProcessor.Amp">
            <summary>
            Process all Reconstruction calls on GPU using C++ AMP and any DirectX11 compatible GPU 
            (real-time reconstruction on suitable hardware)
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.Reconstruction">
            <summary>
            Reconstruction encapsulates reconstruction volume creation updating and meshing functions.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.Reconstruction.volume">
            <summary>
            The native reconstruction interface wrapper.
            </summary>
        </member>
        <member name="F:Microsoft.Kinect.Fusion.Reconstruction.disposed">
            <summary>
            Track whether Dispose has been called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.#ctor(Microsoft.Kinect.Fusion.INuiFusionReconstruction)">
            <summary>
            Initializes a new instance of the Reconstruction class.
            Default constructor used to initialize with the native Reconstruction volume object.
            </summary>
            <param name="volume">
            The native Reconstruction volume object to be encapsulated.
            </param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.Finalize">
            <summary>
            Finalizes an instance of the Reconstruction class.
            This destructor will run only if the Dispose method does not get called.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.FusionCreateReconstruction(Microsoft.Kinect.Fusion.ReconstructionParameters,Microsoft.Kinect.Fusion.ReconstructionProcessor,System.Int32,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Initialize a Kinect Fusion 3D Reconstruction Volume.
            Voxel volume axis sizes must be greater than 0 and a multiple of 32. A Kinect camera 
            is also required to be connected.
            </summary>
            <param name="reconstructionParameters">
            The Reconstruction parameters to define the size and shape of the reconstruction volume.
            </param>
            <param name="reconstructionProcessorType">
            the processor type to be used for all calls to the reconstruction volume object returned
            from this function.
            </param>
            <param name="deviceIndex">Set this variable to an explicit zero-based device index to use
            a specific GPU as enumerated by FusionDepthProcessor.GetDeviceInfo, or set to -1 to 
            automatically select the default device for a given processor type.
            </param>
            <param name="initialWorldToCameraTransform">
            The initial camera pose of the reconstruction volume with respect to the world origin. 
            Pass identity as the default camera pose. 
            </param>
            <returns>The Reconstruction instance.</returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="reconstructionParameters"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="reconstructionParameters"/> parameter's <c>VoxelX</c>,
            <c>VoxelY</c>, or <c>VoxelZ</c> member is not a greater than 0 and multiple of 32.
            Thrown when the <paramref name="deviceIndex"/> parameter is less than -1 or greater 
            than the number of available devices for the respective processor type.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown when the memory required for the Reconstruction volume processing could not be
            allocated.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the Kinect device is not
            connected or the Reconstruction volume is too big so a GPU memory allocation failed, 
            or the call failed for an unknown reason.
            </exception>
            <remarks>
            Users can select which device the processing is performed on with
            the <paramref name="reconstructionProcessorType"/> parameter. For those with multiple GPUs
            the <paramref name="deviceIndex"/> parameter also enables users to explicitly configure
            on which device the reconstruction volume is created.
            Note that this function creates a default world-volume transform. To set a non-default
            transform call ResetReconstruction with an appropriate Matrix4. This default transformation
            is a combination of translation in X,Y to locate the world origin at the center of the front
            face of the reconstruction volume cube, and scaling by the voxelsPerMeter reconstruction
            parameter to convert from the world coordinate system to volume voxel indices.
            </remarks>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.ResetReconstruction(Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Clear the volume, and set a new world-to-camera transform (camera view pose) or identity. 
            This internally sets the default world-to-volume transform. where the Kinect camera is
            translated in X,Y to the center of the front face of the volume cube, looking into the cube, 
            and the world coordinates are scaled to volume indices according to the voxels per meter 
            setting.
            </summary>
            <param name="initialWorldToCameraTransform">
            The initial camera pose with respect to the world origin. 
            Pass identity as the default camera pose. 
            </param>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.ResetReconstruction(Microsoft.Kinect.Fusion.Matrix4,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Clear the reconstruction volume, and set a world-to-camera transform (camera view pose)
            and a world-to-volume transform.
            The world-volume transform expresses the location and orientation of the world coordinate 
            system origin in volume coordinates and the scaling of the world coordinates to
            volume indices. In practice, this controls where the reconstruction volume appears in the
            real world with respect to the world origin position, or with respect to the camera if 
            identity is passed for the initial world-to-camera transform (as the camera and world 
            origins then coincide).
            To create your own world-volume transformation first get the current transform by calling
            GetCurrentWorldToVolumeTransform then either modify the matrix directly or multiply
            with your own similarity matrix to alter the volume translation or rotation with respect
            to the world coordinate system. Note that other transforms such as skew are not supported.
            To reset the volume while keeping the same world-volume transform, first get the current
            transform by calling GetCurrentWorldToVolumeTransform and pass this Matrix4 as the
            <paramref name="worldToVolumeTransform"/> parameter when calling this reset
            function.
            </summary>
            <param name="initialWorldToCameraTransform">
            The initial camera pose with respect to the world origin. 
            Pass identity as the default camera pose. 
            </param>
            <param name="worldToVolumeTransform">A  Matrix4 instance, containing the world to volume
            transform.
            </param>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.AlignDepthFloatToReconstruction(Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Int32,Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Single@,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Aligns a depth float image to the Reconstruction volume to calculate the new camera pose.
            This camera tracking method requires a Reconstruction volume, and updates the internal 
            camera pose if successful. The maximum image resolution supported in this function is 640x480.
            </summary>
            <param name="depthFloatFrame">The depth float frame to be processed.</param>
            <param name="maxAlignIterationCount">
            The maximum number of iterations of the algorithm to run. 
            The minimum value is 1. Using only a small number of iterations will have a faster runtime,
            however, the algorithm may not converge to the correct transformation.
            </param>
            <param name="deltaFromReferenceFrame">
            Optionally, a pre-allocated float image frame, to be filled with information about how
            well each observed pixel aligns with the passed in reference frame. This may be processed
            to create a color rendering, or may be used as input to additional vision algorithms such 
            as object segmentation. These residual values are normalized -1 to 1 and represent the 
            alignment cost/energy for each pixel. Larger magnitude values (either positive or negative)
            represent more discrepancy, and lower values represent less discrepancy or less information
            at that pixel.
            Note that if valid depth exists, but no reconstruction model exists behind the depth   
            pixels, 0 values indicating perfect alignment will be returned for that area. In contrast,
            where no valid depth occurs 1 values will always be returned. Pass null if not required.
            </param>
            <param name="alignmentEnergy">
            A float to receive a value describing how well the observed frame aligns to the model with
            the calculated pose. A larger magnitude value represent more discrepancy, and a lower value
            represent less discrepancy. Note that it is unlikely an exact 0 (perfect alignment) value 
            will ever be returned as every frame from the sensor will contain some sensor noise.
            </param>
            <param name="worldToCameraTransform">
            The best guess of the camera pose (usually the camera pose result from the last
            AlignPointClouds or AlignDepthFloatToReconstruction).
            </param>
            <returns>
            Returns true if successful; return false if the algorithm encountered a problem aligning
            the input depth image and could not calculate a valid transformation.
            </returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="depthFloatFrame"/> parameter is an incorrect image size.
            Thrown when the <paramref name="maxAlignIterationCount"/> parameter is less than 1 or
            an incorrect value.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
            <remarks>
            Note that this function is designed primarily for tracking either with static scenes when
            performing environment reconstruction, or objects which move rigidly when performing object
            reconstruction from a static camera. Consider using the function AlignPointClouds instead 
            if tracking failures occur due to parts of a scene which move non-rigidly or should be 
            considered as outliers, although in practice, such issues are best avoided by carefully 
            designing or constraining usage scenarios wherever possible.
            </remarks> 
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.GetCurrentWorldToCameraTransform">
            <summary>
            Get current internal world-to-camera transform (camera view pose).
            </summary>
            <returns>The current world to camera pose.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.GetCurrentWorldToVolumeTransform">
            <summary>
            Get current internal world-to-volume transform.
            Note: A right handed coordinate system is used, with the origin of the volume (i.e. voxel 0,0,0)
            at the top left of the front plane of the cube. Similar to bitmap images with top left origin, 
            +X is to the right, +Y down, and +Z is now forward from origin into the reconstruction volume.
            The default transform is a combination of translation in X,Y to locate the world origin at the
            center of the front face of the reconstruction volume cube (with the camera looking onto the
            volume along +Z), and scaling by the voxelsPerMeter reconstruction parameter to convert from
            world coordinate system to volume voxel indices.
            </summary>
            <returns>The current world to volume transform. This is a similarity transformation
            that converts world coordinates to volume coordinates.</returns>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.IntegrateFrame(Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Int32,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Integrates depth float data into the reconstruction volume from the passed
            camera pose.
            Note: this function will also set the internal camera pose.
            </summary>
            <param name="depthFloatFrame">The depth float frame to be integrated.</param>
            <param name="maxIntegrationWeight">
            A parameter to control the temporal smoothing of depth integration. Minimum value is 1.
            Lower values have more noisy representations, but objects that move integrate and 
            disintegrate faster, so are suitable for more dynamic environments. Higher values
            integrate objects more slowly, but provides finer detail with less noise.</param>
            <param name="worldToCameraTransform">
            The camera pose (usually the camera pose result from the last 
            FusionDepthProcessor.AlignPointClouds or AlignDepthFloatToReconstruction).
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="maxIntegrationWeight"/> parameter is less than 1 or
            greater than the maximum unsigned short value.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.ProcessFrame(Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Int32,System.Int32,System.Single@,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            A high-level function to process a depth frame through the Kinect Fusion pipeline.
            Specifically, this performs processing equivalent to the following functions for each frame:
            <para>
            1) AlignDepthFloatToReconstruction
            2) IntegrateFrame
            </para>
            If there is a tracking error in the AlignDepthFloatToReconstruction stage, no depth data 
            integration will be performed, and the camera pose will remain unchanged.
            The maximum image resolution supported in this function is 640x480.
            </summary>
            <param name="depthFloatFrame">The depth float frame to be processed.</param>
            <param name="maxAlignIterationCount">
            The maximum number of iterations of the align camera tracking algorithm to run.
            The minimum value is 1. Using only a small number of iterations will have a faster
            runtime, however, the algorithm may not converge to the correct transformation.
            </param>
            <param name="maxIntegrationWeight">
            A parameter to control the temporal smoothing of depth integration. Lower values have
            more noisy representations, but objects that move appear and disappear faster, so are
            suitable for more dynamic environments. Higher values integrate objects more slowly,
            but provides finer detail with less noise.
            </param>
            <param name="alignmentEnergy">
            A float to receive a value describing how well the observed frame aligns to the model with
            the calculated pose. A larger magnitude value represent more discrepancy, and a lower value
            represent less discrepancy. Note that it is unlikely an exact 0 (perfect alignment) value 
            will ever be returned as every frame from the sensor will contain some sensor noise.
            </param>
            <param name="worldToCameraTransform">
            The best guess of the latest camera pose (usually the camera pose result from the last
            process call).
            </param>
            <returns>
            Returns true if successful; return false if the algorithm encountered a problem aligning
            the input depth image and could not calculate a valid transformation.
            </returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="depthFloatFrame"/> parameter is an incorrect image size.
            Thrown when the <paramref name="maxAlignIterationCount"/> parameter is less than 1 or
            greater than the maximum unsigned short value.
            Thrown when the <paramref name="maxIntegrationWeight"/> parameter is less than 1 or 
            greater than the maximum unsigned short value.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            or the call failed for an unknown reason.
            </exception>
            <remarks>
            Users may also optionally call the low-level functions individually, instead of calling this
            function, for more control. However, this function call will be faster due to the integrated 
            nature of the calls. After this call completes, if a visible output image of the reconstruction
            is required, the user can call CalculatePointCloud and then FusionDepthProcessor.ShadePointCloud.
            </remarks>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.CalculatePointCloud(Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Calculate a point cloud by raycasting into the reconstruction volume, returning the point
            cloud containing 3D points and normals of the zero-crossing dense surface at every visible
            pixel in the image from the given camera pose.
            This point cloud can be used as a reference frame in the next call to
            FusionDepthProcessor.AlignPointClouds, or passed to FusionDepthProcessor.ShadePointCloud
            to produce a visible image output.
            The <paramref name="pointCloudFrame"/> can be an arbitrary image size, for example, enabling
            you to calculate point clouds at the size of your window and then create a visible image by
            calling FusionDepthProcessor.ShadePointCloud and render this image, however, be aware that 
            large images will be expensive to calculate.
            </summary>
            <param name="pointCloudFrame">
            The pre-allocated point cloud frame, to be filled by raycasting into the reconstruction volume.
            Typically used as the reference frame with the FusionDepthProcessor.AlignPointClouds function
            or for visualization by calling FusionDepthProcessor.ShadePointCloud.
            </param>
            <param name="worldToCameraTransform">
            The world to camera transform (camera pose) to raycast from.
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="pointCloudFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.CalculateMesh(System.Int32)">
            <summary>
            Export a polygon mesh of the zero-crossing dense surfaces from the reconstruction volume.
            </summary>
            <param name="voxelStep">
            The step value in voxels for sampling points to use in the volume when exporting a mesh, which
            determines the final resolution of the mesh. Use higher values for lower resolution meshes. 
            voxelStep must be greater than 0 and smaller than the smallest volume axis voxel resolution. 
            To mesh the volume at its full resolution, use a step value of 1.
            Note: Any value higher than 1 for this parameter runs the risk of missing zero crossings, and
            hence missing surfaces or surface details.
            </param>
            <returns>Returns the mesh object created by Kinect Fusion.</returns>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="voxelStep"/> parameter is less than 1 or 
            greater than the maximum unsigned integer value or the smallest volume axis resolution.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if the CPU memory required for mesh calculation could not be allocated.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.ExportVolumeBlock(System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int16[])">
            <summary>
            Export a part or all of the reconstruction volume as a short array. 
            The surface boundary occurs where the tri-linearly interpolated voxel values have a zero crossing
            (i.e. when an interpolation crosses from positive to negative or vice versa). A voxel value of 
            0x8000 indicates that a voxel is uninitialized and has no valid data associated with it.
            </summary>
            <param name="sourceOriginX">The reconstruction volume voxel index in the X axis from which the 
            extraction should begin. This value must be greater than or equal to 0 and less than the
            reconstruction volume X axis voxel resolution.</param>
            <param name="sourceOriginY">The reconstruction volume voxel index in the Y axis from which the 
            extraction should begin. This value must be greater than or equal to 0 and less than the
            reconstruction volume Y axis voxel resolution.</param>
            <param name="sourceOriginZ">The reconstruction volume voxel index in the Z axis from which the 
            extraction should begin. This value must be greater than or equal to 0 and less than the
            reconstruction volume Z axis voxel resolution.</param>
            <param name="destinationResolutionX">The X axis resolution/width of the new voxel volume to return
            in the array. This value must be greater than 0 and less than or equal to the current volume X 
            axis voxel resolution. The final count of (sourceOriginX+(destinationResolutionX*voxelStep) must 
            not be greater than the current reconstruction volume X axis voxel resolution.</param>
            <param name="destinationResolutionY">The Y axis resolution/height of the new voxel volume to return
            in the array. This value must be greater than 0 and less than or equal to the current volume Y 
            axis voxel resolution. The final count of (sourceOriginY+(destinationResolutionY*voxelStep) must 
            not be greater than the current reconstruction volume Y axis voxel resolution.</param>
            <param name="destinationResolutionZ">The Z axis resolution/depth of the new voxel volume to return
            in the array. This value must be greater than 0 and less than or equal to the current volume Z 
            axis voxel resolution. The final count of (sourceOriginZ+(destinationResolutionZ*voxelStep) must 
            not be greater than the current reconstruction volume Z axis voxel resolution.</param>
            <param name="voxelStep">The step value in integer voxels for sampling points to use in the
            volume when exporting. The value must be greater than 0 and less than the smallest 
            volume axis voxel resolution. To export the volume at its full resolution, use a step value of 1. 
            Use higher step values to skip voxels and return the new volume as if there were a lower effective 
            resolution volume. For example, when exporting with a destination resolution of 320^3, setting 
            voxelStep to 2 would actually cover a 640^3 voxel are a(destinationResolution*voxelStep) in the 
            source reconstruction, but the data returned would skip every other voxel in the original volume.
            NOTE:  Any value higher than 1 for this value runs the risk of missing zero crossings, and hence
            missing surfaces or surface details.</param>
            <param name="volumeBlock">A pre-allocated short array to be filled with 
            volume data. The number of elements in this user array should be allocated as:
            (destinationResolutionX * destinationResolutionY * destinationResolutionZ) 
            To access the voxel located at x,y,z use pVolume[z][y][x], or index as 1D array for a particular
            voxel(x,y,z) as follows: with pitch = x resolution, slice = (y resolution * pitch)
            unsigned int index = (z * slice)  + (y * pitch) + x;
            Note: A right handed coordinate system is used, with the origin of the volume (i.e. voxel 0,0,0) 
            at the top left of the front plane of the cube. Similar to bitmap images with top left origin, 
            +X is to the right, +Y down, and +Z is forward from origin into the reconstruction volume.
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="volumeBlock"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="volumeBlock"/> parameter length is not equal to
            (<paramref name="destinationResolutionX"/> * <paramref name="destinationResolutionY"/> *
            <paramref name="destinationResolutionZ"/>).
            Thrown when a sourceOrigin or destinationResolution parameter less than 1 or
            greater than the maximum unsigned short value.
            Thrown when the (sourceOrigin+(destinationResolution*voxelStep) calculation was
            greater than the current reconstruction volume voxel resolution along an axis.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if the CPU memory required for volume export could not be allocated.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.ImportVolumeBlock(System.Int16[])">
            <summary>
            Import a reconstruction volume as a short array. 
            This array must equal the size of the current initialized reconstruction volume.
            </summary>
            <param name="volumeBlock">A pre-allocated short array filled with volume data.
            The number of elements in this user array should be allocated as:
            (sourceResolutionX * sourceResolutionY * sourceResolutionZ) 
            To access the voxel located at x,y,z use pVolume[z][y][x], or index as 1D array for a particular
            voxel(x,y,z) as follows: with pitch = x resolution, slice = (y resolution * pitch)
            unsigned int index = (z * slice)  + (y * pitch) + x;
            Note: A right handed coordinate system is used, with the origin of the volume (i.e. voxel 0,0,0) 
            at the top left of the front plane of the cube. Similar to bitmap images with top left origin, 
            +X is to the right, +Y down, and +Z is forward from origin into the reconstruction volume.
            </param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="volumeBlock"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="volumeBlock"/> parameter length is not equal to
            the existing initialized volume.
            </exception>
            <exception cref="T:System.OutOfMemoryException">
            Thrown if the CPU memory required for volume export could not be allocated.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected,
            a GPU memory allocation failed or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.DepthToDepthFloatFrame(System.UInt16[],Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Single,System.Single,System.Boolean)">
            <summary>
            Converts Kinect depth frames in unsigned short format to depth frames in float format 
            representing distance from the camera in meters (parallel to the optical center axis).
            Note: <paramref name="depthImageData"/> and <paramref name="depthFloatFrame"/> must
            be the same pixel resolution. This version of the function runs on the GPU.
            </summary>
            <param name="depthImageData">The source depth data.</param>
            <param name="depthFloatFrame">A depth float frame, to be filled with depth.</param>
            <param name="minDepthClip">The minimum depth threshold. Values below this will be set to 0.</param>
            <param name="maxDepthClip">The maximum depth threshold. Values above this will be set to 1000.</param>
            <param name="mirrorDepth">Set true to mirror depth, false so the image appears correct if viewing
            the Kinect camera from behind.</param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="depthImageData"/> or 
            <paramref name="depthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="depthImageData"/> or
            <paramref name="depthFloatFrame"/> parameter is an incorrect image size, or the 
            kernelWidth is an incorrect size.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
            <remarks>
            The min and max depth clip values enable clipping of the input data, for example, to help
            isolate particular objects or surfaces to be reconstructed. Note that the thresholds return 
            different values when a depth pixel is outside the threshold - pixels inside minDepthClip will
            will be returned as 0 and ignored in processing, whereas pixels beyond maxDepthClip will be set
            to 1000 to signify a valid depth ray with depth beyond the set threshold. Setting this far-
            distance flag is important for reconstruction integration in situations where the camera is
            static or does not move significantly, as it enables any voxels closer to the camera
            along this ray to be culled instead of persisting (as would happen if the pixels were simply 
            set to 0 and ignored in processing). Note that when reconstructing large real-world size volumes,
            be sure to set large maxDepthClip distances, as when the camera moves around, any voxels in view
            which go beyond this threshold distance from the camera will be removed.
            </remarks>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.SmoothDepthFloatFrame(Microsoft.Kinect.Fusion.FusionFloatImageFrame,Microsoft.Kinect.Fusion.FusionFloatImageFrame,System.Int32,System.Single)">
            <summary>
            Spatially smooth a depth float image frame using edge-preserving filtering on GPU. 
            </summary>
            <param name="depthFloatFrame">A source depth float frame.</param>
            <param name="smoothDepthFloatFrame">A depth float frame, to be filled with smoothed 
            depth.</param>
            <param name="kernelWidth">Smoothing Kernel Width. Valid values are  1,2,3 
            (for 3x3,5x5,7x7 smoothing kernel block size respectively).</param>
            <param name="distanceThreshold">A distance difference range that smoothing occurs in.
            Pixels with neighboring pixels outside this distance range will not be smoothed 
            (larger values indicate discontinuity/edge). Must be greater than 0.</param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="smoothDepthFloatFrame"/> or 
            <paramref name="depthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="smoothDepthFloatFrame"/> or
            <paramref name="depthFloatFrame"/> parameter is an incorrect image size, or the 
            kernelWidth is an incorrect size.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.AlignPointClouds(Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,System.Int32,Microsoft.Kinect.Fusion.FusionColorImageFrame,System.Single@,Microsoft.Kinect.Fusion.Matrix4@)">
            <summary>
            The AlignPointClouds function uses an on GPU iterative algorithm to align two sets of
            overlapping oriented point clouds and calculate the camera's relative pose.
            All images must be the same size and have the same camera parameters. 
            </summary>
            <param name="referencePointCloudFrame">A reference point cloud frame.</param>
            <param name="observedPointCloudFrame">An observerd point cloud frame.</param>
            <param name="maxAlignIterationCount">The number of iterations to run.</param>
            <param name="deltaFromReferenceFrame">
            Optionally, a pre-allocated color image frame, to be filled with color-coded data
            from the camera tracking. This may be used as input to additional vision algorithms such as
            object segmentation. Values vary depending on whether the pixel was a valid pixel used in
            tracking (inlier) or failed in different tests (outlier). 0xff000000 indicates an invalid 
            input vertex (e.g. from 0 input depth), or one where no correspondences occur between point
            cloud images. Outlier vertices rejected due to too large a distance between vertices are 
            coded as 0xff008000. Outlier vertices rejected due to to large a difference in normal angle
            between point clouds are coded as 0xff800000. Inliers are color shaded depending on the 
            residual energy at that point, with more saturated colors indicating more discrepancy
            between vertices and less saturated colors (i.e. more white) representing less discrepancy,
            or less information at that pixel. Pass null if this image is not required.
            </param>
            <param name="alignmentEnergy">A value describing
            how well the observed frame aligns to the model with the calculated pose (mean distance between
            matching points in the point clouds). A larger magnitude value represent more discrepancy, and 
            a lower value represent less discrepancy. Note that it is unlikely an exact 0 (perfect alignment) 
            value will ever/ be returned as every frame from the sensor will contain some sensor noise. 
            Pass NULL to ignore this parameter.</param>
            <param name="referenceToObservedTransform">The initial guess at the transform. This is 
            updated on tracking success, or returned as identity on failure.</param>
            <returns>
            Returns true if successful; return false if the algorithm encountered a problem aligning
            the input depth image and could not calculate a valid transformation.
            </returns>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="referencePointCloudFrame"/> or 
            <paramref name="observedPointCloudFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="referencePointCloudFrame"/> or
            <paramref name="observedPointCloudFrame"/> or <paramref name="deltaFromReferenceFrame"/>
            parameter is an incorrect image size, or the iterations parameter is not greater than 0.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.SetAlignDepthFloatToReconstructionReferenceFrame(Microsoft.Kinect.Fusion.FusionFloatImageFrame)">
            <summary>
            Set a reference depth frame to be used internally to help with tracking when calling 
            AlignDepthFloatToReconstruction to calculate a new camera pose. This function should
            only be called when not using the default tracking behavior of Kinect Fusion.
            </summary>
            <param name="referenceDepthFloatFrame">A previous depth float frame where align was
            successful (and hence same functionality as AlignDepthFloatToReconstruction), 
            or a ray-casted model depth.</param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="referenceDepthFloatFrame"/> parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="referenceDepthFloatFrame"/> parameter is an incorrect 
            image size.</exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the call failed for an unknown reason.
            </exception>
            <remarks>
            AlignDepthFloatToReconstruction internally saves the last depth frame it was passed and
            uses this image to help it track when called the next time. For example, this can be used
            if you are reconstructing and lose track, then want to re-start tracking from a different
            (known) location without resetting the volume. To enable the tracking to succeed you
            could perform a raycast from the new location to get a depth image (by calling 
            CalculatePointCloudAndDepth) then call this set function with the depth image, before 
            calling AlignDepthFloatToReconstruction.
            </remarks>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.CalculatePointCloudAndDepth(Microsoft.Kinect.Fusion.FusionPointCloudImageFrame,Microsoft.Kinect.Fusion.FusionFloatImageFrame,Microsoft.Kinect.Fusion.Matrix4)">
            <summary>
            Calculate a point cloud and depth image by raycasting into the reconstruction volume. 
            This returns the point cloud containing 3D points and normals of the zero-crossing dense 
            surface at every visible pixel in the image from the given camera pose, the depth
            to the surface.
            </summary>
            <param name="pointCloudFrame">A  point cloud frame, to be filled by raycasting into the 
            reconstruction volume. Typically used as the reference frame with the 
            FusionDepthProcessor.AlignPointClouds function or for visualization by calling 
            FusionDepthProcessor.ShadePointCloud.</param>
            <param name="depthFloatFrame">A floating point depth frame, to be filled with floating point
            depth in meters to the raycast surface. This image must be identical
            in size, and camera parameters to the <paramref name="pointCloudFrame"/> parameter.</param>
            <param name="worldToCameraTransform">The world-to-camera transform (camera pose) to raycast
            from.</param>
            <exception cref="T:System.ArgumentNullException">
            Thrown when the <paramref name="pointCloudFrame"/> or <paramref name="depthFloatFrame"/> 
            parameter is null.
            </exception>
            <exception cref="T:System.ArgumentException">
            Thrown when the <paramref name="pointCloudFrame"/> or<paramref name="depthFloatFrame"/>
            parameter is an incorrect image size.
            </exception>
            <exception cref="T:System.InvalidOperationException">
            Thrown when the Kinect Runtime could not be accessed, the device is not connected
            or the call failed for an unknown reason.
            </exception>
            <remarks>
            This point cloud can then be used as a reference frame in the next call to 
            FusionDepthProcessor.AlignPointClouds, or passed to FusionDepthProcessor.ShadePointCloud
            to produce a visible image output.The depth image can be used as a reference frame for
            AlignDepthFloatToReconstruction by calling SetAlignDepthFloatToReconstructionReferenceFrame 
            to enable a greater range of tracking. The <paramref name="pointCloudFrame"/> and 
            <paramref name="depthFloatFrame"/> parameters can be an arbitrary image size, for example, 
            enabling you to calculate point clouds at the size of your UI window and then create a visible
            image by calling FusionDepthProcessor.ShadePointCloud and render this image, however, be aware
            that the calculation of high resolution images will be expensive in terms of runtime.
            </remarks>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.Dispose">
            <summary>
            Disposes the Reconstruction.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Reconstruction.Dispose(System.Boolean)">
            <summary>
            Frees all memory associated with the Reconstruction.
            </summary>
            <param name="disposing">Whether the function was called from Dispose.</param>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.Resources">
            <summary>
              A strongly-typed resource class, for looking up localized strings, etc.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.ResourceManager">
            <summary>
              Returns the cached ResourceManager instance used by this class.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.Culture">
            <summary>
              Overrides the current thread's CurrentUICulture property for all
              resource lookups using this strongly typed resource class.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.BadIndex">
            <summary>
              Looks up a localized string similar to The specified index is out of range..
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.DeviceNotConnected">
            <summary>
              Looks up a localized string similar to Kinect is not connected..
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.GenericException">
            <summary>
              Looks up a localized string similar to This API has returned an exception from an HRESULT: 0x{0:X}.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.GpuInitializeFailed">
            <summary>
              Looks up a localized string similar to Failed to initialize GPU or the GPU is not capable of running Kinect Fusion..
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.ImageDataLengthMismatch">
            <summary>
              Looks up a localized string similar to The data buffer length must match the length required by the associated The data buffer length must match the length required by the associated ImageFormat..
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.InvalidArguments">
            <summary>
              Looks up a localized string similar to One or more input arguments were invalid, or arguments outside valid range..
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.LibraryInvocationFailed">
            <summary>
              Looks up a localized string similar to Unable to invoke library &apos;{0}&apos;..
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.LoadFunctionFailed">
            <summary>
              Looks up a localized string similar to Unable to load function &apos;{0}&apos;..
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.NuiFeatureNotInitialized">
            <summary>
              Looks up a localized string similar to Kinect SDK Runtime cannot be accessed..
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.OutOfCpuMemory">
            <summary>
              Looks up a localized string similar to Out of memory when allocating..
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Resources.OutOfGpuMemory">
            <summary>
              Looks up a localized string similar to Out of memory when allocating on GPU..
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.Vector3">
            <summary>
            Vector3 is a 3-element vector, typically used for storing 3D points or normals.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Vector3.op_Equality(Microsoft.Kinect.Fusion.Vector3,Microsoft.Kinect.Fusion.Vector3)">
            <summary>
            Compares two Vector3 objects for equality.
            </summary>
            <param name="vector1">The first Vector3 to compare.</param>
            <param name="vector2">The second Vector3 to compare.</param>
            <returns>Returns true if both Vector3 objects are equal and false otherwise.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Vector3.op_Inequality(Microsoft.Kinect.Fusion.Vector3,Microsoft.Kinect.Fusion.Vector3)">
            <summary>
            Compares two Vector3 objects for inequality.
            </summary>
            <param name="vector1">The first Vector3 to compare.</param>
            <param name="vector2">The second Vector3 to compare.</param>
            <returns>Returns true if the Vector3 objects are not equal and false otherwise.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Vector3.Equals(Microsoft.Kinect.Fusion.Vector3)">
            <summary>
            Compares two Vector3 objects for equality.
            </summary>
            <param name="other">The Vector3 to compare with.</param>
            <returns>Returns true if both Vector3 objects are equal and false otherwise.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Vector3.Equals(System.Object)">
            <summary>
            Compares two Vector3 objects for equality.
            </summary>
            <param name="obj">The object to compare.</param>
            <returns>Returns true if they are equal and false otherwise.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.Vector3.GetHashCode">
            <summary>
            Gets the hash code for a given Vector3.
            </summary>
            <returns>The calculated hash code.</returns>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Vector3.X">
            <summary>
            Gets or sets the X element.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Vector3.Y">
            <summary>
            Gets or sets the Y element.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.Vector3.Z">
            <summary>
            Gets or sets the Z element.
            </summary>
        </member>
        <member name="T:Microsoft.Kinect.Fusion.ReconstructionParameters">
            <summary>
            This class is used to setup the volume parameters.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ReconstructionParameters.#ctor(System.Single,System.Int32,System.Int32,System.Int32)">
            <summary>
            Initializes a new instance of the ReconstructionParameters class.
            </summary>
            <param name="voxelsPerMeter">Voxels per meter. Must be greater than 0.</param>
            <param name="voxelsX">Number of Voxels in X. Must be greater than 0 and a multiple of 32.</param>
            <param name="voxelsY">Number ofVoxels in Y. Must be greater than 0 and a multiple of 32.</param>
            <param name="voxelsZ">Number ofVoxels in Z. Must be greater than 0 and a multiple of 32.</param>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ReconstructionParameters.#ctor">
            <summary>
            Prevents a default instance of the ReconstructionParameters class from being created.
            The declaration of the default constructor is used for marshaling operations.
            </summary>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ReconstructionParameters.GetHashCode">
            <summary>
            Calculates the hash code of the ReconstructionParameters.
            </summary>
            <returns>The hash code.</returns>
        </member>
        <member name="M:Microsoft.Kinect.Fusion.ReconstructionParameters.Equals(Microsoft.Kinect.Fusion.ReconstructionParameters)">
            <summary>
            Determines if the two objects are equal.
            </summary>
            <param name="other">The object to compare to.</param>
            <returns>This method returns true if they are equal and false otherwise.</returns>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.ReconstructionParameters.VoxelsPerMeter">
            <summary>
            Gets the number of Voxels per Meter.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.ReconstructionParameters.VoxelsX">
            <summary>
            Gets the size of the reconstruction volume in voxels in the X axis.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.ReconstructionParameters.VoxelsY">
            <summary>
            Gets the size of the reconstruction volume in voxels in the Y axis.
            </summary>
        </member>
        <member name="P:Microsoft.Kinect.Fusion.ReconstructionParameters.VoxelsZ">
            <summary>
            Gets the size of the reconstruction volume in voxels in the Z axis.
            </summary>
        </member>
    </members>
</doc>
